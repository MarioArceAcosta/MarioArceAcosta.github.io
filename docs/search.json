[
  {
    "objectID": "xcite_dashboard.html",
    "href": "xcite_dashboard.html",
    "title": "XCITE Center for Teaching and Learning Dashboard on Academic Technology Usage",
    "section": "",
    "text": "University of California, Riverside\n\n\nA year-long project consisting of:\n\nGathering data from academic learning tool vendors\nCleaning, preparing, and organizing data into a database\nAnalyzing and presenting the analysis in a data reporting tool through dashboards\nDisseminating the insights and key metrics to supervisors and stakeholders"
  },
  {
    "objectID": "xcite_dashboard.html#a-year-long-initiative-as-a-data-analytics-intern",
    "href": "xcite_dashboard.html#a-year-long-initiative-as-a-data-analytics-intern",
    "title": "XCITE Center for Teaching and Learning Dashboard on Academic Technology Usage",
    "section": "",
    "text": "University of California, Riverside\n\n\nA year-long project consisting of:\n\nGathering data from academic learning tool vendors\nCleaning, preparing, and organizing data into a database\nAnalyzing and presenting the analysis in a data reporting tool through dashboards\nDisseminating the insights and key metrics to supervisors and stakeholders"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "",
    "text": "This study estimates the extent of education–occupation mismatch and the associated wage penalties for undocumented college graduates. Using data from the American Community Survey (ACS), we classify workers as vertically mismatched (higher educational attainment than is typical for the occupation) or horizontally mismatched (field of degree is not typical for the occupation). Because the ACS does not identify undocumented status, we train machine learning models on the Survey of Income and Program Participation (SIPP) and use the predictions to impute status in the ACS. This approach enables new analyses of labor market outcomes for undocumented college graduates in nationally representative surveys. Results indicate that undocumented status is linked to higher rates of both vertical and horizontal mismatch, as well as wage penalties of roughly 4–7 percent. These penalties are smaller among the DACA-eligible and in states with more inclusive immigrant policy climates."
  },
  {
    "objectID": "research.html#loading-data",
    "href": "research.html#loading-data",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.1 Loading Data",
    "text": "3.1 Loading Data\n\n\nCode\ndata_path &lt;- \"G:/Shared drives/Undocu Research/Data\"\nfigures_path &lt;- \"G:/Shared drives/Undocu Research/Output/Figures\"\nsetwd(data_path)\n\nsipp08_2 &lt;- read_csv(\"(Step 1 output) Core_TM SIPP 2008 Wave 2.csv\")"
  },
  {
    "objectID": "research.html#sipp-data-cleaning-and-recoding",
    "href": "research.html#sipp-data-cleaning-and-recoding",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.2 SIPP Data Cleaning and Recoding",
    "text": "3.2 SIPP Data Cleaning and Recoding\n\n\nCode\nsipp08_2 &lt;- sipp08_2 %&gt;%\n  mutate(\n    undocu_entry = as.factor(ifelse(timstat==\"Other\", 1, 0)),\n    undocu_likely = as.factor(ifelse(timstat==\"Other\" & eadjust==\"No\", 1, 0)),\n    education = case_when(\n      eeducate == \"10th Grade\"  | eeducate == \"11th Grade\" | eeducate == \"12th grade, no diploma\" | eeducate == \"1st, 2nd, 3rd, or 4th grade\" | eeducate == \"5th Or 6th Grade\" | eeducate == \"7th Or 8th Grade\" | eeducate == \"9th Grade\" | eeducate == \"Less Than 1st Grade\"~ \"No HS diploma\",\n      eeducate == \"Diploma or certificate from a\" | eeducate == \"High School Graduate - (diploma\" ~ \"HS diploma\",\n      eeducate == \"Some college, but no degree\" ~ \"Some college\",\n      eeducate ==\"Associate (2-yr) college degree\" ~ \"Associate's\",\n      eeducate == \"Bachelor's degree (for example:\" ~ \"Bachelor's\",\n      eeducate == \"Master's degree (For example: MA,\" ~ \"Master's\",\n      eeducate == \"Doctorate degree (for example:\" ~ \"PhD\",\n      TRUE ~ \"Unknown\" # Default case\n    ),\n    yrsed = case_when(\n      eeducate == \"10th Grade\"~10,\n      eeducate == \"11th Grade\"~11,\n      eeducate == \"12th grade, no diploma\" | eeducate == \"Diploma or certificate from a\" | eeducate == \"High School Graduate - (diploma\" | eeducate == \"Some college, but no degree\" ~12,\n      eeducate == \"1st, 2nd, 3rd, or 4th grade\"~2.5,\n      eeducate == \"5th Or 6th Grade\"~5.5,\n      eeducate == \"7th Or 8th Grade\"~7.5,\n      eeducate == \"9th Grade\"~9,\n      eeducate == \"Less Than 1st Grade\"~0,\n      eeducate ==\"Associate (2-yr) college degree\"~14,\n      eeducate == \"Bachelor's degree (for example:\"~16,\n      eeducate == \"Master's degree (For example: MA,\"~17.5,\n      eeducate == \"Doctorate degree (for example:\"~22,\n      eeducate == \"Professional School degree (for\"~16,\n      TRUE ~ NA\n    ),\n    college = as.factor(ifelse(eeducate==\"Bachelor's degree (for example:\" | eeducate==\"Master's degree (For example: MA,\" | eeducate==\"Doctorate degree (for example:\", 1, 0)),\n    hs_only = as.factor(ifelse(eeducate==\"Some college, but no degree\" | eeducate== \"Associate (2-yr) college degree\" | eeducate==\"High School Graduate - (diploma\" | eeducate==\"Diploma or certificate from a\", 1, 0)),\n    immig_yr = case_when(\n      tmoveus == \"1961\"~1961,\n      tmoveus == \"1961-1968\"~1966,\n      tmoveus == \"1969-1973\"~1971,\n      tmoveus == \"1974-1978\"~1976,\n      tmoveus == \"1979-1980\"~1980,\n      tmoveus == \"1981-1983\"~1982,\n      tmoveus == \"1984-1985\"~1984,\n      tmoveus == \"1986-1988\"~1987,\n      tmoveus == \"1989-1990\"~1989,\n      tmoveus == \"1991-1992\"~1991,\n      tmoveus == \"1993-1994\"~1993,\n      tmoveus == \"1995-1996\"~1995,\n      tmoveus == \"1997-1998\"~1998,\n      tmoveus == \"1999\"~1999,\n      tmoveus == \"2000\"~2000,\n      tmoveus == \"2001\"~2001,\n      tmoveus == \"2002-2003\"~2002,\n      tmoveus == \"2004\"~2004,\n      tmoveus == \"2005\"~2005,\n      tmoveus == \"2006\"~2006,\n      tmoveus == \"2007\"~2007,\n      tmoveus == \"2008-2009\"~2009,\n      TRUE ~ 0 # Default case\n    ),\n    married = as.factor(ifelse(ems==\"Married, spouse absent\" | ems==\"Married, spouse present\", 1, 0)),\n    english_difficult = as.factor(ifelse(ehowwell==\"Not at all\" | ehowwell==\"Not well\", 1, 0)),\n    nonfluent = as.factor(ifelse(ehowwell==\"Not at all\" | ehowwell==\"Not well\", 1, 0)),\n    english_home = as.factor(ifelse(tlang1==\"Not in Universe\", 1, 0)),\n    spanish_hispanic_latino = as.factor(ifelse(eorigin==\"Yes\", 1, 0)),\n    medicaid = as.factor(ifelse(rcutyp57==\"Yes, covered\", 1, 0)),\n    household_size = ehhnumpp,\n    race = case_when(\n      erace==\"Asian alone\" ~ \"Asian\",\n      erace==\"Black alone\" ~ \"Black\",\n      erace==\"White alone\" ~ \"White\",\n      erace==\"Residual\" ~ \"Other\",\n      TRUE ~ \"Unknown\"\n    ),\n    fem = as.factor(ifelse(esex==\"Female\", 1, 0)),\n    asian = as.factor(ifelse(erace==\"Asian alone\", 1, 0)),\n    black = as.factor(ifelse(erace==\"Black alone\", 1, 0)),\n    white = as.factor(ifelse(erace==\"White alone\", 1, 0)),\n    other_race = as.factor(ifelse(erace==\"Residual\", 1, 0)),\n    employed = as.factor(ifelse(rmesr==\"With a job at least 1 but not all\" | rmesr==\"With a job entire month, absent\" | rmesr==\"With a job entire month, worked\", 1, 0)),\n    years_us = rhcalyr - immig_yr,\n    citizen = as.factor(ifelse(ecitizen==\"Yes\", 1, 0)),\n    cit_spouse = as.factor(cit_spouse),\n    poverty = as.factor(ifelse(thearn&lt;rhpov, 1, 0)),\n    armed_forces = as.factor(ifelse(eafnow==\"Yes\" | eafever==\"Yes\", 1, 0)),\n    health_ins= as.factor(ifelse(rcutyp57==\"Yes, covered\" | rcutyp58==\"Yes, covered\" , 1, 0)),\n    medicare = as.factor(ifelse(ecrmth==\"Yes, covered\", 1, 0)),\n    social_security = as.factor(ifelse(rcutyp01==\"Yes, covered\" | rcutyp03==\"Yes, covered\", 1, 0)),\n    central_latino = as.factor(ifelse(tbrstate==\"Central America\" & eorigin==\"Yes\", 1, 0)),\n    bpl_usa = as.factor(ifelse(ebornus==\"Yes\", 1, 0)),\n    bpl_asia = as.factor(ifelse(tbrstate == \"Eastern Asia\"| tbrstate == \"South Central Asia\"| tbrstate == \"South East Asia, West Asia,\", 1, 0)),\n    top_ten_states = as.factor(ifelse(tfipsst==\"California\" | tfipsst==\"Texas\" | tfipsst==\"Florida\" | tfipsst==\"New Jersey\" | tfipsst==\"Illinois\" | tfipsst==\"New York\" | tfipsst==\"North Carolina\" | tfipsst==\"Georgia\" | tfipsst==\"Washington\" | tfipsst==\"Arizona\", 1, 0))\n  )\n\nsipp08_2$bpl_foreign &lt;- as.factor(ifelse(sipp08_2$bpl_usa==1, 0, 1))\nsipp08_2$undocu_likely &lt;- replace(sipp08_2$undocu_likely, sipp08_2$immig_yr &lt;= 1961, 0)\nsipp08_2$years_us &lt;- ifelse(sipp08_2$years_us == 2008 | sipp08_2$years_us == 2009 | sipp08_2$years_us == -1 , NA, sipp08_2$years_us)\nsipp08_2$tage &lt;- replace(sipp08_2$tage, sipp08_2$tage == \"Less than 1 full year old\", 0)\nsipp08_2$age &lt;- as.numeric(sipp08_2$tage)\nsipp08_2$undocu_likely &lt;- replace(sipp08_2$undocu_likely, sipp08_2$armed_forces==1 | sipp08_2$social_security==1, 0 )\nsipp08_2$undocu_logical &lt;- as.factor(ifelse(sipp08_2$citizen==0 & (sipp08_2$armed_forces==0 | sipp08_2$medicare==0 | sipp08_2$social_security==0), 1, 0))\nsipp08_2$id &lt;- seq_len(nrow(sipp08_2))\n\n\n# Define column sets for different analyses\nvariable_lists &lt;- list(\n  base = c(\"undocu_likely\", \"central_latino\", \"bpl_asia\", \"medicaid\", \"age\", \"fem\", \n           \"married\", \"cit_spouse\", \"nonfluent\", \"spanish_hispanic_latino\", \n           \"household_size\", \"poverty\", \"asian\", \"black\", \"white\", \"other_race\", \n           \"employed\", \"years_us\", \"yrsed\"),\n  \n  descriptive = c(\"undocu_likely\", \"undocu_logical\", \"bpl_foreign\", \"medicaid\", \n                  \"central_latino\", \"bpl_asia\", \"age\", \"fem\", \"married\", \"cit_spouse\", \n                  \"nonfluent\", \"spanish_hispanic_latino\", \"household_size\", \"poverty\", \n                  \"asian\", \"black\", \"white\", \"other_race\", \"employed\", \"years_us\", \"yrsed\")\n\n)"
  },
  {
    "objectID": "research.html#subsamples",
    "href": "research.html#subsamples",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.3 Subsamples",
    "text": "3.3 Subsamples\n\n\nCode\n# Create analysis datasets\ncreate_datasets &lt;- function(data, variables) {\n  datasets &lt;- list()\n  \n  # Filter for undocu_logical == 1\n  undocu_filter &lt;- data[data$undocu_logical == 1, ]\n  \n  datasets$dTable &lt;- as.data.frame(undocu_filter[, variables$descriptive]) %&gt;%\n    mutate_at(vars(-undocu_likely), as.numeric) %&gt;%\n    na.omit()\n  \n  datasets$sample &lt;- as.data.frame(undocu_filter[, variables$base]) %&gt;%\n    na.omit()\n  \n  datasets$knn &lt;- as.data.frame(undocu_filter[, variables$base]) %&gt;%\n    mutate_at(vars(-undocu_likely), as.numeric) %&gt;%\n    na.omit()\n  \n  \n  # Demographic subsets (college graduates only)\n  datasets$noncit &lt;- data[data$citizen == 0 & data$college == 1, ]\n  datasets$central_latino &lt;- data[data$central_latino == 1 & data$college == 1, ]\n  datasets$spanish_hispanic_latino &lt;- data[data$spanish_hispanic_latino == 1 & data$college == 1, ]\n  datasets$top_states &lt;- data[data$top_ten_states == 1 & data$college == 1, ]\n  \n  return(datasets)\n}\n\n\n# Create all analysis datasets\nall_data &lt;- create_datasets(sipp08_2, variable_lists)\n\n# Create all college analysis datasets\nall_data_college &lt;- lapply(all_data, function(x) {\n  if (is.data.frame(x) && \"college\" %in% names(x)) {\n    x %&gt;% filter(college == 1)\n  } else {\n    x  \n  }\n})\n\n## Save for later use across documents\nsaveRDS(sipp08_2, \"data/sipp08_2.rds\")\nsaveRDS(all_data, \"data/all_data.rds\")\nsaveRDS(all_data_college, \"data/all_data_college.rds\")\nsaveRDS(variable_lists, \"data/variable_lists.rds\")"
  },
  {
    "objectID": "research.html#logistic",
    "href": "research.html#logistic",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.4 Logistic",
    "text": "3.4 Logistic\n\n\nCode\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\n\nset.seed(1)\ntrain_index_logistic &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_logistic &lt;- all_data$sample[train_index_logistic, ]\ntest_logistic &lt;- all_data$sample[-train_index_logistic, ]\n\n## Create trainControl object\ncontrol &lt;- trainControl(\n    method = \"cv\",\n    number = 10,  \n    summaryFunction = twoClassSummary,\n    classProbs = TRUE,\n    sampling = \"up\",\n    allowParallel = FALSE\n)\n\n## Train glm with custom trainControl\nlogistic_model &lt;- train(undocu_likely ~ age + fem + married + cit_spouse + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + medicaid + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, train_logistic,\n               method = \"glm\",\n               trControl = control,\n               metric = 'ROC')\n\np_logistic &lt;- predict(logistic_model, test_logistic)\n\n\n# Generate confusion matrix\nlogistic_matrix &lt;- confusionMatrix(p_logistic, test_logistic$undocu_likely, positive = \"X1\")\nprint(logistic_matrix)\nsummary_logistic&lt;-summary(logistic_model)$coefficients[,c(1,4)]\nsummary_logistic\nxtable(summary_logistic, digits=4)\n\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nlogistic.preds = predict(logistic_model, newdata = test_logistic, type = \"prob\")[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nlogistic.prediction = prediction(logistic.preds, test_logistic$undocu_likely)\n\nlogistic.pr = performance(logistic.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 676 141\n        X1 335 300\n                                          \n               Accuracy : 0.6722          \n                 95% CI : (0.6474, 0.6963)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 0.978           \n                                          \n                  Kappa : 0.3104          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6803          \n            Specificity : 0.6686          \n         Pos Pred Value : 0.4724          \n         Neg Pred Value : 0.8274          \n             Prevalence : 0.3037          \n         Detection Rate : 0.2066          \n   Detection Prevalence : 0.4373          \n      Balanced Accuracy : 0.6745          \n                                          \n       'Positive' Class : X1              \n                                          \n                             Estimate     Pr(&gt;|z|)\n(Intercept)               1.086274760 2.403930e-05\nage                      -0.028615899 2.805518e-22\nfem1                     -0.107628855 1.062181e-01\nmarried1                 -0.059375631 3.964547e-01\ncit_spouse1              -0.220240756 4.178604e-03\nnonfluent1                0.292738271 1.213362e-04\nspanish_hispanic_latino1  0.024379055 8.948373e-01\ncentral_latino1           0.935938662 1.932298e-07\nbpl_asia1                -0.212503302 1.758857e-01\nmedicaid1                -0.394646972 1.362742e-04\nhousehold_size            0.006487852 7.258096e-01\npoverty1                  0.335671026 9.558128e-06\nasian1                   -0.014682482 9.441694e-01\nblack1                   -0.217738645 2.824772e-01\nwhite1                   -0.206672671 2.291539e-01\nemployed1                 0.104813209 1.496649e-01\nyears_us                 -0.041445286 3.879564e-20\nyrsed                     0.003842108 7.005681e-01"
  },
  {
    "objectID": "research.html#k-nearest-neighbors",
    "href": "research.html#k-nearest-neighbors",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.5 K-Nearest Neighbors",
    "text": "3.5 K-Nearest Neighbors\n\n\nCode\nset.seed(1)\nlevels(all_data$knn$undocu_likely) &lt;- make.names(levels(all_data$knn$undocu_likely))\ntrain_index_knn &lt;- createDataPartition(all_data$knn$undocu_likely, p = 0.7, list = FALSE)\ntrain_knn &lt;- all_data$knn[train_index_knn, ]\ntest_knn &lt;- all_data$knn[-train_index_knn, ]\n\n\n\nknn_model &lt;- train(undocu_likely ~., data = train_knn, method = \"knn\", \n                       trControl = control, \n                       tuneLength = 10,\n                       metric = 'ROC',\n                       preProcess = c('center', 'scale'))\n\nknn_model\n\npredict_knn &lt;- predict(knn_model, test_knn)\n\nknn_matrix &lt;- confusionMatrix(predict_knn, test_knn$undocu_likely, positive = \"X1\")\nprint(knn_matrix)\n\n\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nknn.preds = predict(knn_model, newdata = test_knn, type = \"prob\")[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nknn.prediction = prediction(knn.preds, test_knn$undocu_likely)\n\nknn.pr = performance(knn.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n\nk-Nearest Neighbors \n\n3391 samples\n  18 predictor\n   2 classes: 'X0', 'X1' \n\nPre-processing: centered (18), scaled (18) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 3051, 3052, 3052, 3052, 3052, 3052, ... \nAddtional sampling using up-sampling prior to pre-processing\n\nResampling results across tuning parameters:\n\n  k   ROC        Sens       Spec     \n   5  0.6541396  0.5872881  0.6207058\n   7  0.6635782  0.5974576  0.6430825\n   9  0.6688536  0.6000000  0.6488331\n  11  0.6799451  0.6118644  0.6644137\n  13  0.6834141  0.6067797  0.6625000\n  15  0.6891838  0.6038136  0.6585512\n  17  0.6862166  0.6110169  0.6566374\n  19  0.6831830  0.6008475  0.6498786\n  21  0.6892569  0.6088983  0.6606329\n  23  0.6887947  0.6059322  0.6683346\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 21.\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 637 137\n        X1 374 304\n                                          \n               Accuracy : 0.6481          \n                 95% CI : (0.6229, 0.6727)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2774          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6893          \n            Specificity : 0.6301          \n         Pos Pred Value : 0.4484          \n         Neg Pred Value : 0.8230          \n             Prevalence : 0.3037          \n         Detection Rate : 0.2094          \n   Detection Prevalence : 0.4669          \n      Balanced Accuracy : 0.6597          \n                                          \n       'Positive' Class : X1              \n                                          \n\n\n\nThe number of trees in the forest\nThe number of features to consider at any given split: \\(m_{try}\\)\nThe complexity of each tree\nThe sampling scheme\nThe splitting rule to use during tree construction\nand (2) typically have the largest impact on predictive accuracy and should always be tuned. (3) and (4) tend to have marginal impact on predictive accuracy but are still worth exploring. They also have the ability to influence computational efficiency. (5) tends to have the smallest impact on predictive accuracy and is used primarily to increase computational efficiency."
  },
  {
    "objectID": "research.html#random-forest",
    "href": "research.html#random-forest",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.6 Random Forest",
    "text": "3.6 Random Forest\n\n\nCode\n### You may use the following code to speed up code being ran (Use all cores except one)\n# cl &lt;- makePSOCKcluster(detectCores() - 1) \n# registerDoParallel(cl)\n\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\n\n\ntrain_index_rf &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_rf &lt;- all_data$sample[train_index_rf, ]\ntest_rf &lt;- all_data$sample[-train_index_rf, ]\n\n\ntrain_rf &lt;- train_rf %&gt;%\n  select(-undocu_likely, undocu_likely)\n\n#Manual search by create 10 folds and repeat 3 times\n#control_rf &lt;- trainControl(method = 'repeatedcv',\n                        #number = 10,\n                       # repeats = 3,\n                        #search = 'grid')\n\ntunegrid &lt;- expand.grid(mtry = c(2,4,8,12),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = 1)\n\ncontrol_up &lt;- trainControl(\n    method = \"cv\",\n    number = 10,  \n    summaryFunction = twoClassSummary,\n    classProbs = TRUE,\n    sampling = \"up\",\n    allowParallel = FALSE\n)\n\n\nset.seed(1)\nrf_model &lt;- train(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed,\n               data = train_rf,\n               method = \"ranger\",\n               trControl = control_up,\n               tuneLength = 5,\n               importance = \"impurity\",\n               metric = 'ROC')\nprint(rf_model)\nplot(rf_model)\n\n\n\n\n\n\n\n\n\nCode\np_rf &lt;- predict(rf_model, test_rf)\n\nrf_matrix &lt;- confusionMatrix(p_rf, test_rf$undocu_likely, positive=\"X1\")\nrf_matrix\n\nfeature_importance &lt;- vip(rf_model, num_features = 19, bar = FALSE)\ngrid.arrange(feature_importance, nrow = 1)\n\n\n\n\n\n\n\n\n\nCode\nall_data$dTable$undocu_logit &lt;- predict(logistic_model, all_data$sample)\nall_data$dTable$undocu_knn &lt;- predict(knn_model, all_data$knn)\nall_data$dTable$undocu_rf &lt;- predict(rf_model, all_data$sample)\nsetwd(\"G:/Shared drives/Undocu Research/Data\")\nwrite.csv(all_data$dTable, \"SIPP_dTable.csv\", row.names = FALSE)\n\n\nrf_model$finalModel$num.trees\n\n# Access specific parameters\nrf_optimal_trees &lt;- rf_model$bestTune$mtry\ncat(\"Optimal mtry:\", rf_optimal_trees, \"\\n\")\n\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nrf.preds = predict(rf_model, newdata = test_rf, type = \"prob\", n.trees = rf_optimal_trees)[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nrf.prediction = prediction(rf.preds, test_rf$undocu_likely)\n\nrf.pr = performance(rf.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n# stopCluster(cl) # Optional code part of parallel processing\n\n\nRandom Forest \n\n3391 samples\n  18 predictor\n   2 classes: 'X0', 'X1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 3052, 3052, 3051, 3052, 3052, 3052, ... \nAddtional sampling using up-sampling\n\nResampling results across tuning parameters:\n\n  mtry  splitrule   ROC        Sens       Spec     \n   2    gini        0.7531242  0.7135593  0.6586165\n   2    extratrees  0.7409146  0.6809322  0.6896098\n   6    gini        0.7496400  0.8088983  0.5102128\n   6    extratrees  0.7483606  0.7940678  0.5286034\n  10    gini        0.7421022  0.8233051  0.4947069\n  10    extratrees  0.7421380  0.8110169  0.4966019\n  14    gini        0.7317369  0.8063559  0.4888910\n  14    extratrees  0.7391064  0.8088983  0.4898058\n  18    gini        0.7297986  0.8101695  0.4889190\n  18    extratrees  0.7369086  0.8105932  0.4908140\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1.\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 715 166\n        X1 296 275\n                                          \n               Accuracy : 0.6818          \n                 95% CI : (0.6572, 0.7057)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 0.8897          \n                                          \n                  Kappa : 0.3054          \n                                          \n Mcnemar's Test P-Value : 1.954e-09       \n                                          \n            Sensitivity : 0.6236          \n            Specificity : 0.7072          \n         Pos Pred Value : 0.4816          \n         Neg Pred Value : 0.8116          \n             Prevalence : 0.3037          \n         Detection Rate : 0.1894          \n   Detection Prevalence : 0.3933          \n      Balanced Accuracy : 0.6654          \n                                          \n       'Positive' Class : X1              \n                                          \n[1] 500\nOptimal mtry: 2"
  },
  {
    "objectID": "research.html#gradient-boosted-trees",
    "href": "research.html#gradient-boosted-trees",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.7 Gradient-Boosted Trees",
    "text": "3.7 Gradient-Boosted Trees\n\n\nCode\nset.seed(1)\n## Data partitioning for caret library\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\ntrain_index_gbm &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_gbm &lt;- all_data$sample[train_index_gbm, ]\ntest_gbm &lt;- all_data$sample[-train_index_gbm, ]\n\n## Data partitioning for gbm library\ntrain_gbm_num &lt;-  train_gbm\ntrain_gbm_num$undocu_likely &lt;- as.numeric(train_gbm$undocu_likely) - 1      \ntest_gbm_num &lt;-  test_gbm\ntest_gbm_num$undocu_likely &lt;- as.numeric(test_gbm$undocu_likely) - 1 \n\ngbm_model &lt;- gbm(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_gbm_num, \n                 verbose = FALSE,\n                 cv.folds = 10,\n                 n.trees = 1766,\n                 shrinkage = 0.005,\n                 interaction.depth = 6)\n\n\n## Model summary and optimal number of trees\ngbm_model\noptimal_trees_gbm_two &lt;- gbm.perf(gbm_model, method = \"cv\", plot.it = FALSE)\ncat(\"Optimal number of trees:\", optimal_trees_gbm_two, \"\\n\")\n\n\nsetwd(figures_path)\n## Feature importance\nfeature_importance_gbm &lt;- vip(gbm_model, num_features = 19, bar = FALSE)\nggsave(\"gbm_feature_importance.png\", feature_importance_gbm, width = 22, height = 16, units = \"cm\", dpi = 300)\ngrid.arrange(feature_importance_gbm, nrow = 1)\n\n\n\n\n\n\n\n\n\nDistribution not specified, assuming bernoulli ...\ngbm(formula = undocu_likely ~ age + fem + married + cit_spouse + \n    medicaid + nonfluent + spanish_hispanic_latino + central_latino + \n    bpl_asia + household_size + poverty + asian + black + white + \n    other_race + employed + years_us + yrsed, data = train_gbm_num, \n    n.trees = 1766, interaction.depth = 6, shrinkage = 0.005, \n    cv.folds = 10, verbose = FALSE)\nA gradient boosted model with bernoulli loss function.\n1766 iterations were performed.\nThe best cross-validation iteration was 1765.\nThere were 18 predictors of which 18 had non-zero influence.\nOptimal number of trees: 1765 \n\n\n\n\nCode\n## Metrics for Figure 2 comparison (gbm library model)\n# Make predictions on test set\ngbm.preds = predict(gbm_model, newdata = test_gbm_num, type = \"response\", n.trees = optimal_trees_gbm_two)\n# Create prediction object for ROCR\ngbm.prediction = prediction(gbm.preds, test_gbm_num$undocu_likely)\n\ngbm.pr = performance(gbm.prediction,\"prec\",\"rec\") # Precision-Recall curve\ngbm.roc = performance(gbm.prediction,\"tpr\",\"fpr\")   # ROC curve \ngbm.auc &lt;- performance(gbm.prediction,\"auc\")@y.values[[1]] # AUC value\n\ngbm_auc_pr &lt;- trapz(\n  as.vector(gbm.pr@x.values)[[1]][-1], \n  as.vector(gbm.pr@y.values)[[1]][-1]\n)\n\n\nrf.pr = performance(rf.prediction,\"prec\",\"rec\") # Precision-Recall curve\nrf.roc = performance(rf.prediction,\"tpr\",\"fpr\") \n\n\nsetwd(\"G:/Shared drives/Undocu Research/Output/Figures\")\npng(\"roc_curves.png\", width = 22*100, height = 16*100, res = 300)\n\nplot(gbm.roc, main = \"ROC Curves for contending models\", col = \"blue\", lwd = 2) \nlines(rf.roc@x.values[[1]], rf.roc@y.values[[1]], col = \"red\", lwd = 2)\n\nlegend(x = 0.6, y = 1, \n       col = c(\"white\", \"blue\", \"red\"),\n       lwd = c(3, 3, 3), \n       lty = c(0, 1, 2),\n       legend = c(\"Model:\", \"Gradient boosting\", \"Random Forest\"),\n       bty = 'n', cex = 0.95, ncol = 1, seg.len = 6)\n\nabline(a = 0, b = 1, lty = 2, col = \"gray\")\n\ndev.off()\n\n# Plot the first curve\nplot(rf.pr, col = \"blue\", lwd = 2, main = \"Precision-Recall Curve\")\n# Add the second curve\nlines(gbm.pr@x.values[[1]], gbm.pr@y.values[[1]], col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\npng \n  2"
  },
  {
    "objectID": "research.html#ml-performance-evaluation",
    "href": "research.html#ml-performance-evaluation",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "3.8 ML Performance Evaluation",
    "text": "3.8 ML Performance Evaluation\n\n\nCode\nsetwd(\"G:/Shared drives/Undocu Research/Output/Figures\")\n# ==============================================================================\n# PREPARE PRECISION-RECALL DATA FOR ALL MODELS\n# ==============================================================================\n\n# Start with boosted trees as baseline\ngbm_pr_data &lt;- data.frame(\n  recall = unlist(gbm.pr@x.values), \n  precision = unlist(gbm.pr@y.values)\n)\n\n# Interpolate other models to match boosted trees recall values\nrf_pr_data &lt;- data.frame(\n  recall = approx(unlist(rf.pr@x.values), unlist(rf.pr@y.values), \n                 xout = gbm_pr_data$recall)$x,\n  precision = approx(unlist(rf.pr@x.values), unlist(rf.pr@y.values), \n                    xout = gbm_pr_data$recall)$y\n)\n\nlogistic_pr_data &lt;- data.frame(\n  recall = approx(unlist(logistic.pr@x.values), unlist(logistic.pr@y.values), \n                 xout = gbm_pr_data$recall)$x,\n  precision = approx(unlist(logistic.pr@x.values), unlist(logistic.pr@y.values), \n                    xout = gbm_pr_data$recall)$y\n)\n\nknn_pr_data &lt;- data.frame(\n  recall = approx(unlist(knn.pr@x.values), unlist(knn.pr@y.values), \n                 xout = gbm_pr_data$recall)$x,\n  precision = approx(unlist(knn.pr@x.values), unlist(knn.pr@y.values), \n                    xout = gbm_pr_data$recall)$y\n)\n\n# ==============================================================================\n# COMBINE ALL DATA\n# ==============================================================================\n\nall_pr_data &lt;- gbm_pr_data\n\n# Add precision values for other models\nall_pr_data$gbm_precision &lt;- gbm_pr_data$precision\nall_pr_data$rf_precision &lt;- rf_pr_data$precision\nall_pr_data$logistic_precision &lt;- logistic_pr_data$precision\nall_pr_data$knn_precision &lt;- knn_pr_data$precision\n\n# Calculate precision differences relative to boosted trees\nall_pr_data$gbm_precision_difference &lt;- gbm_pr_data$precision - all_pr_data$precision\nall_pr_data$rf_precision_difference &lt;- rf_pr_data$precision - all_pr_data$precision\nall_pr_data$logistic_precision_difference &lt;- logistic_pr_data$precision - all_pr_data$precision\nall_pr_data$knn_precision_difference &lt;- knn_pr_data$precision - all_pr_data$precision\n\n# ==============================================================================\n# PLOT 1: PRECISION-RECALL CURVES FOR ALL MODELS\n# ==============================================================================\n\n#pdf(\"undocumented_pr_curves.pdf\", family=\"Times\", width=9)\npng(\"undocumented_pr_curves.png\", width = 22*100, height = 16*100, res = 300, family = \"Times\")\n\nop &lt;- par(family = \"serif\")\n\nplot(all_pr_data$recall, all_pr_data$precision, type = \"l\", pch = 19,\n     col = \"yellow\", xlab = \"Recall\", ylab = \"Precision\",\n     lwd = 3, ylim=c(0, 1), xlim = c(0,1), lty = 1, cex.lab=1.5, cex.axis=1)\n\nlines(all_pr_data$recall, all_pr_data$rf_precision, type = \"l\", \n      col = \"blue\", lwd = 3, lty = 2)\nlines(all_pr_data$recall, all_pr_data$logistic_precision, type = \"l\",\n      col = \"red\", lwd = 3, lty = 3)\nlines(all_pr_data$recall, all_pr_data$knn_precision, type = \"l\",\n      col = \"green\", lwd = 3, lty = 4)\n\nlegend(x = 0.5, y = 1, \n       col = c(\"white\", \"yellow\", \"blue\", \"red\", \"green\"),\n       lwd = c(3, 3, 3, 3, 3, 3), \n       lty = c(0, 1, 2, 3, 4, 5),\n       legend = c(\"Model:\", \"Gradient-Boosting Trees\", \"Random Forest\", \"Logistic\", \"KNN\"),\n       bty = 'n', cex = 1.25, ncol = 1, seg.len = 6)\n\n# Add random baseline\ntotal_undocu &lt;- sum(test_gbm_num$undocu_likely == 1)\nrandom_baseline &lt;- total_undocu / nrow(test_gbm_num)\nabline(h = random_baseline, lty = 2, col = \"gray\")\n# Add text label for random baseline\ntext(0.5, random_baseline + 0.05, \n     paste(\"Random baseline:\", round(random_baseline, 3)), \n     cex = 0.9, col = \"gray30\")\n\n\ndev.off()\n\n\n### Relative Pr-Re curves ###\n\npng(\"undocumented_precision_differences.png\", width = 22*100, height = 16*100, res = 300, family = \"Times\")\n\nop &lt;- par(family = \"serif\")\n\nplot(all_pr_data$recall, all_pr_data$rf_precision_difference, type = \"l\", pch = 19, \n     col = \"blue\", xlab = \"Recall Relative to Boosted Trees\", ylab = \"Precision\", \n     lwd = 3, ylim = c(-0.3, 0.1), xlim = c(0,1), lty = 2, cex.lab=1.5, cex.axis=1)\n\nlines(all_pr_data$recall, all_pr_data$logistic_precision_difference, type = \"l\", \n      col = \"red\", lwd = 3, lty = 3)\nlines(all_pr_data$recall, all_pr_data$knn_precision_difference, type = \"l\",\n      col = \"green\", lwd = 3, lty = 4)\n\nlegend(x = 0.5, y = -0.1, \n       col = c(\"white\", \"blue\", \"red\", \"green\"),\n       lwd = c(3, 3, 3, 3), \n       lty = c(0, 2, 3, 4),\n       legend = c(\"Model:\", \"Random Forest\", \"Logistic\", \"KNN\"),\n       bty = 'n', cex = 1.25, ncol = 1, seg.len = 5)\n\nabline(h = 0, lty = 2, col = \"gray\")\n# Add text label for random baseline\ntext(0.5, 0.05, \n     paste(\"GBM baseline:\", round(0, 3)), \n     cex = 0.9, col = \"gray30\")\n\ndev.off()\n\n\npng \n  2 \npng \n  2 \n\n\n\nSensitivity: \\(\\frac{TP}{TP + FN}\\)\nSpecificity: \\(\\frac{TN}{TN + FP}\\)\nPrecision / Positive-predictive value: \\(\\frac{TP}{TP + FP}\\)\nAccuracy: \\(\\frac{TP + TN}{total}\\)\n\nSource: https://bradleyboehmke.github.io/HOML/process.html"
  },
  {
    "objectID": "research.html#loading-data-1",
    "href": "research.html#loading-data-1",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "4.1 Loading Data",
    "text": "4.1 Loading Data\n\n\nCode\nsetwd(data_path)\n\nACS_variables &lt;- c(\"hisp\", \"spanish\", \"race\", \"bpld\", \"empstat\", \"poverty\", \"married\", \"cit_spouse\", \"nonfluent\", \"bpl_asia\", \"asian\", \"black\", \"white\", \"fem\", \"numprec\", \"yrsusa1\", \"hinscare\", \"hinscaid\", \"undocu\", \"bpl_usa\", \"age\", \"yrsed\")  \n\n\nACS &lt;- read_csv(\"ML Training Sample.csv\")\nView(ACS)"
  },
  {
    "objectID": "research.html#preparing-main-data-set-to-introduce-ml-results",
    "href": "research.html#preparing-main-data-set-to-introduce-ml-results",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "4.2 Preparing main data set to introduce ML results",
    "text": "4.2 Preparing main data set to introduce ML results\n\n\nCode\nACS_sipp &lt;- ACS %&gt;%\n  mutate(\n    spanish_hispanic_latino = as.factor(ifelse(hisp==1 | spanish==1, 1, 0)),\n    central_latino = as.factor(ifelse((hisp==1) & (bpld==\"belize/british honduras\" | bpld==\"costa rica\" | bpld==\"el salvador\" | bpld==\"guatemala\" | bpld==\"honduras\" | bpld==\"nicaragua\" | bpld==\"panama\" | bpld==\"mexico\"), 1, 0)),\n    black = as.factor(ifelse(race==\"black/african america\", 1, 0)),\n    white = as.factor(ifelse(race==\"white\", 1, 0)),\n    employed = as.factor(ifelse(empstat==1, 1, 0)),\n    poverty = as.factor(ifelse(poverty&lt;100, 1, 0)),\n    married = as.factor(married),\n    cit_spouse = as.factor(cit_spouse),\n    nonfluent = as.factor(nonfluent),\n    bpl_asia = as.factor(bpl_asia),\n    asian = as.factor(asian),\n    fem = as.factor(fem),\n    household_size = numprec,\n    #Change SIPP 0 years_us to NA\n    years_us = yrsusa1,\n    medicare = as.factor(ifelse(hinscare==\"yes\", 1, 0)),\n    medicaid = as.factor(ifelse(hinscaid==\"has insurance through medicaid\", 1, 0))\n    \n  )\nACS_sipp$years_us &lt;- replace(ACS_sipp$years_us, ACS_sipp$yrsusa1==\"n/a or less than one year\" & ACS_sipp$bpl_usa==1, NA)\nACS_sipp$years_us &lt;- replace(ACS_sipp$years_us, ACS_sipp$yrsusa1==\"n/a or less than one year\" & ACS_sipp$bpl_usa==0, 0)\nACS_sipp$years_us &lt;- as.numeric(ACS_sipp$years_us)\n\nACS_sipp$other_race &lt;- as.factor(ifelse(ACS$black!=1 & ACS$white!=1 & ACS$asian!=1, 1, 0))\nACS_sipp_na &lt;- ACS_sipp  %&gt;%\n  filter(undocu==1, !is.na(years_us),!is.na(medicaid), !is.na(age),!is.na(fem), !is.na(married),!is.na(cit_spouse), !is.na(nonfluent),  !is.na(spanish_hispanic_latino), !is.na(central_latino), !is.na(bpl_asia), !is.na(household_size), !is.na(poverty), !is.na(asian), !is.na(black), !is.na(white), !is.na(other_race), !is.na(employed),  !is.na(yrsed))\n\nACS_cols_numeric &lt;- c(\"central_latino\", \"bpl_asia\", \"medicaid\", \"age\", \"fem\", \"married\", \"cit_spouse\", \"nonfluent\", \"spanish_hispanic_latino\", \"household_size\", \"poverty\", \"asian\", \"black\", \"white\", \"other_race\", \"employed\", \"years_us\",\"yrsed\")\n\nACS_sipp_knn &lt;- ACS_sipp_na %&gt;%\n  mutate_at(ACS_cols_numeric, as.numeric)"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Conferences & Presentations",
    "section": "",
    "text": "Washington, D.C. — April 12, 2025\n\n\nThis study estimates the extent of education–occupation mismatch and the associated wage penalties for undocumented college graduates. Using data from the American Community Survey (ACS), we classify workers as vertically mismatched (higher educational attainment than is typical for the occupation) or horizontally mismatched (field of degree is not typical for the occupation). Because the ACS does not identify undocumented status, we train machine learning models on the Survey of Income and Program Participation (SIPP) and use the predictions to impute status in the ACS. This approach enables new analyses of labor market outcomes for undocumented college graduates in nationally representative surveys. Results indicate that undocumented status is linked to higher rates of both vertical and horizontal mismatch, as well as wage penalties of roughly 4–7 percent. These penalties are smaller among the DACA-eligible and in states with more inclusive immigrant policy climates.\n\n\n\n\n\n Download PDF"
  },
  {
    "objectID": "presentations.html#abstract",
    "href": "presentations.html#abstract",
    "title": "Conferences & Presentations",
    "section": "",
    "text": "This study estimates the extent of education–occupation mismatch and the associated wage penalties for undocumented college graduates. Using data from the American Community Survey (ACS), we classify workers as vertically mismatched (higher educational attainment than is typical for the occupation) or horizontally mismatched (field of degree is not typical for the occupation). Because the ACS does not identify undocumented status, we train machine learning models on the Survey of Income and Program Participation (SIPP) and use the predictions to impute status in the ACS. This approach enables new analyses of labor market outcomes for undocumented college graduates in nationally representative surveys. Results indicate that undocumented status is linked to higher rates of both vertical and horizontal mismatch, as well as wage penalties of roughly 4–7 percent. These penalties are smaller among the DACA-eligible and in states with more inclusive immigrant policy climates."
  },
  {
    "objectID": "presentations.html#slides",
    "href": "presentations.html#slides",
    "title": "Conferences & Presentations",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "presentations.html#abstract-1",
    "href": "presentations.html#abstract-1",
    "title": "Conferences & Presentations",
    "section": "Abstract",
    "text": "Abstract\nThis study estimates the extent of education–occupation mismatch and the associated wage penalties for undocumented college graduates. Using data from the American Community Survey (ACS), we classify workers as vertically mismatched (higher educational attainment than is typical for the occupation) or horizontally mismatched (field of degree is not typical for the occupation). Because the ACS does not identify undocumented status, we train machine learning models on the Survey of Income and Program Participation (SIPP) and use the predictions to impute status in the ACS. This approach enables new analyses of labor market outcomes for undocumented college graduates in nationally representative surveys. Results indicate that undocumented status is linked to higher rates of both vertical and horizontal mismatch, as well as wage penalties of roughly 4–7 percent. These penalties are smaller among the DACA-eligible and in states with more inclusive immigrant policy climates."
  },
  {
    "objectID": "presentations.html#slides-1",
    "href": "presentations.html#slides-1",
    "title": "Conferences & Presentations",
    "section": "Slides",
    "text": "Slides\n\n\n Download PDF"
  },
  {
    "objectID": "leading_model.html",
    "href": "leading_model.html",
    "title": "Leading ML Model",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(gbm)      # For Gradient Boosting\nlibrary(class)\nlibrary(caTools)\nlibrary(caret)\nlibrary(ROCR)\nlibrary(vip)  # For feature importance\nlibrary(gridExtra)\nlibrary(doParallel)\nlibrary(xtable)"
  },
  {
    "objectID": "leading_model.html#full-training-sample",
    "href": "leading_model.html#full-training-sample",
    "title": "Leading ML Model",
    "section": "3.1 Full Training Sample",
    "text": "3.1 Full Training Sample\n\n\nCode\n### You may use the following code to speed up code being ran (Use all cores except one)\ncl &lt;- makePSOCKcluster(detectCores() - 1) \nregisterDoParallel(cl)\n\nset.seed(1)\n## Data partitioning for gbm library\ncontrol_up &lt;- trainControl(\n    method = \"cv\",\n    number = 10,  \n    summaryFunction = twoClassSummary,\n    classProbs = TRUE,\n    sampling = \"up\",\n)\n\n## Data partitioning for caret library\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\ntrain_index_gbm &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_gbm &lt;- all_data$sample[train_index_gbm, ]\ntest_gbm &lt;- all_data$sample[-train_index_gbm, ]\n\n\ntrain_gbm_num &lt;-  train_gbm\ntrain_gbm_num$undocu_likely &lt;- as.numeric(train_gbm$undocu_likely) - 1      \ntest_gbm_num &lt;-  test_gbm\ntest_gbm_num$undocu_likely &lt;- as.numeric(test_gbm$undocu_likely) - 1 \n\ngbm_model &lt;- gbm(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_gbm_num, \n                 verbose = FALSE,\n                 cv.folds = 10,\n                 n.trees = 1766,\n                 shrinkage = 0.005,\n                 interaction.depth = 6)\n\n\n## Model summary and optimal number of trees\ngbm_model\n\noptimal_trees_gbm_two &lt;- gbm.perf(gbm_model, method = \"cv\", plot.it = FALSE)\ncat(\"Optimal number of trees:\", optimal_trees_gbm_two, \"\\n\")\n\n\nsetwd(figures_path)\n## Feature importance\nfeature_importance_gbm &lt;- vip(gbm_model, num_features = 19, bar = FALSE)\nggsave(\"gbm_feature_importance.png\", feature_importance_gbm, width = 22, height = 16, units = \"cm\", dpi = 300)\ngrid.arrange(feature_importance_gbm, nrow = 1)\n\n\n\n\n\n\n\n\n\nCode\nstopCluster(cl) # Optional code part of parallel processing\n\n\nDistribution not specified, assuming bernoulli ...\ngbm(formula = undocu_likely ~ age + fem + married + cit_spouse + \n    medicaid + nonfluent + spanish_hispanic_latino + central_latino + \n    bpl_asia + household_size + poverty + asian + black + white + \n    other_race + employed + years_us + yrsed, data = train_gbm_num, \n    n.trees = 1766, interaction.depth = 6, shrinkage = 0.005, \n    cv.folds = 10, verbose = FALSE)\nA gradient boosted model with bernoulli loss function.\n1766 iterations were performed.\nThe best cross-validation iteration was 1765.\nThere were 18 predictors of which 18 had non-zero influence.\nOptimal number of trees: 1765"
  },
  {
    "objectID": "leading_model.html#college-training-sample",
    "href": "leading_model.html#college-training-sample",
    "title": "Leading ML Model",
    "section": "3.2 College Training Sample",
    "text": "3.2 College Training Sample\n\n\nCode\ncl &lt;- makePSOCKcluster(detectCores() - 1) \nregisterDoParallel(cl)\n\nset.seed(1)\n## Data partitioning for gbm library\nall_data_college &lt;- readRDS(\"data/all_data_college.rds\")\nall_data_college$sample$undocu_likely &lt;- as.numeric(all_data_college$sample$undocu_likely)-1\n\ntrain_index_college &lt;- createDataPartition(all_data_college$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_college &lt;- all_data_college$sample[train_index_college, ]\ntest_college &lt;- all_data_college$sample[-train_index_college, ]\n\ncollege_gbm_model &lt;- gbm(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_college, \n                 verbose = FALSE,\n                 cv.folds = 10,\n                 n.trees = 1766,\n                 shrinkage = 0.005, \n                 interaction.depth = 6)\n\n\n## Model summary and optimal number of trees\ncollege_gbm_model\n\noptimal_trees_college &lt;- gbm.perf(college_gbm_model, method = \"cv\", plot.it = FALSE)\ncat(\"Optimal number of trees:\", optimal_trees_college, \"\\n\")\n\n\nsetwd(figures_path)\n## Feature importance\ncollege_gbm_feature_importance &lt;- vip(college_gbm_model, num_features = 19, bar = FALSE)\nggsave(\"college_gbm_feature_importance.png\", college_gbm_feature_importance, width = 22, height = 16, units = \"cm\", dpi = 300)\ngrid.arrange(college_gbm_feature_importance, nrow = 1)\n\n\n\n\n\n\n\n\n\nCode\nstopCluster(cl) # Optional code part of parallel processing\non.exit(try(parallel::stopCluster(cl), silent = TRUE), add = TRUE)\n\n\nDistribution not specified, assuming bernoulli ...\ngbm(formula = undocu_likely ~ age + fem + married + cit_spouse + \n    medicaid + nonfluent + spanish_hispanic_latino + central_latino + \n    bpl_asia + household_size + poverty + asian + black + white + \n    other_race + employed + years_us + yrsed, data = train_college, \n    n.trees = 1766, interaction.depth = 6, shrinkage = 0.005, \n    cv.folds = 10, verbose = FALSE)\nA gradient boosted model with bernoulli loss function.\n1766 iterations were performed.\nThe best cross-validation iteration was 1764.\nThere were 18 predictors of which 18 had non-zero influence.\nOptimal number of trees: 1764"
  },
  {
    "objectID": "leading_model.html#p-quartiles-and-thresholds",
    "href": "leading_model.html#p-quartiles-and-thresholds",
    "title": "Leading ML Model",
    "section": "4.1 P Quartiles and Thresholds",
    "text": "4.1 P Quartiles and Thresholds\n\n\nCode\ntest_gbm_num$p = predict(gbm_model, newdata = test_gbm_num, type = \"response\", n.trees = optimal_trees_gbm_two)\ntest_college$p = predict(gbm_model, newdata = test_college, type = \"response\", n.trees = optimal_trees_gbm_two)\n\n\n# Create quartiles based on probability magnitude\n\ntest_gbm_num$p_quartiles &lt;- cut(test_gbm_num$p, \n                                   breaks = quantile(test_gbm_num$p, probs = c(0, 0.25, 0.5, 0.75, 1)), \n                                   labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), \n                                   include.lowest = TRUE)\ntest_college$p_quartiles &lt;- cut(test_college$p, \n                                   breaks = quantile(test_college$p, probs = c(0, 0.25, 0.5, 0.75, 1)), \n                                   labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), \n                                   include.lowest = TRUE)\n# Check the distributions\ntable(test_gbm_num$p_quartiles)\n\n\n\n Q1  Q2  Q3  Q4 \n363 363 363 363 \n\n\nCode\ntable(test_college$p_quartiles)\n\n\n\n Q1  Q2  Q3  Q4 \n363 363 363 363 \n\n\nCode\n# Proportion of undocu_likely in each quartiles\nquartiles_table_gbm &lt;- test_gbm_num %&gt;%\n  group_by(p_quartiles) %&gt;%\n  summarise(\n    count = sum(undocu_likely == 1),\n    n = n(),\n    actual_proportion = (count / n)\n    )\nquartiles_table_college &lt;- test_college %&gt;%\n  group_by(p_quartiles) %&gt;%\n  summarise(\n    count = sum(undocu_likely == 1),\n    n = n(),\n    actual_proportion = (count / n)\n    )\n\nxtable(quartiles_table_gbm, digits=4)\n\n\n\n  \n\n\n\nCode\nprint(quartiles_table_gbm)\n\n\n# A tibble: 4 × 4\n  p_quartiles count     n actual_proportion\n  &lt;fct&gt;       &lt;int&gt; &lt;int&gt;             &lt;dbl&gt;\n1 Q1             35   363            0.0964\n2 Q2             75   363            0.207 \n3 Q3            136   363            0.375 \n4 Q4            195   363            0.537 \n\n\nCode\nxtable(quartiles_table_college, digits=4)\n\n\n\n  \n\n\n\nCode\nprint(quartiles_table_college)\n\n\n# A tibble: 4 × 4\n  p_quartiles count     n actual_proportion\n  &lt;fct&gt;       &lt;int&gt; &lt;int&gt;             &lt;dbl&gt;\n1 Q1             23   363            0.0634\n2 Q2             74   363            0.204 \n3 Q3            119   363            0.328 \n4 Q4            223   363            0.614 \n\n\nCode\ntop_75_sample_num &lt;- test_gbm_num %&gt;%\n  filter(undocu_likely == 1) %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(\n    rank = row_number(),\n    cumulative_p = rank / n(),\n    top_75 = cumulative_p &lt;= 0.75\n  ) %&gt;%\n  filter(top_75==TRUE)\nthreshold_75_num &lt;- min(top_75_sample_num$p)\n\nundocu_test_num &lt;- test_gbm_num %&gt;%\n  filter(undocu_likely == 1) %&gt;%\n  arrange(desc(p))\n\n# Create GBM high recall (p&gt;threshold), high probability (75% of actual), low probability (Q1)\ntest_gbm_num$predicted_class &lt;- ifelse(test_gbm_num$p &gt;= threshold_75_num, 1, 0)\n\nquartile_thresholds_gbm &lt;- quantile(undocu_test_num$p, probs = c(0, 0.25, 0.5, 0.75, 1))\nprint(quartile_thresholds_gbm)\n\n\n        0%        25%        50%        75%       100% \n0.08044671 0.27554476 0.41349639 0.53716540 0.79673260"
  },
  {
    "objectID": "leading_model.html#precision-recall-curves-full-to-full-full-to-college-college-to-college",
    "href": "leading_model.html#precision-recall-curves-full-to-full-full-to-college-college-to-college",
    "title": "Leading ML Model",
    "section": "4.2 Precision-Recall Curves (Full to Full, Full to College, College to College)",
    "text": "4.2 Precision-Recall Curves (Full to Full, Full to College, College to College)\n\n\nCode\nsetwd(figures_path)\n# ==============================================================================\n# PREPARE PRECISION-RECALL DATA FOR ALL MODELS\n# ==============================================================================\n\n# Start with Full to Full predictions as baseline\ngbm_pr_data_ff &lt;- data.frame(\n  recall = unlist(gbm.pr@x.values), \n  precision = unlist(gbm.pr@y.values)\n)\n\n# Interpolate other models to match boosted trees recall values\ngbm_pr_data_fc &lt;- data.frame(\n  recall = approx(unlist(gbm.pr_fc@x.values), unlist(gbm.pr_fc@y.values), \n                 xout = gbm_pr_data_ff$recall)$x,\n  precision = approx(unlist(gbm.pr_fc@x.values), unlist(gbm.pr_fc@y.values), \n                    xout = gbm_pr_data_ff$recall)$y\n)\n\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\nCode\n# Interpolate other models to match boosted trees recall values\ngbm_pr_data_cc &lt;- data.frame(\n  recall = approx(unlist(gbm.pr_cc@x.values), unlist(gbm.pr_cc@y.values), \n                 xout = gbm_pr_data_ff$recall)$x,\n  precision = approx(unlist(gbm.pr_cc@x.values), unlist(gbm.pr_cc@y.values), \n                    xout = gbm_pr_data_ff$recall)$y\n)\n\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\nCode\n# ==============================================================================\n# COMBINE ALL DATA\n# ==============================================================================\n\nall_pr_data &lt;- gbm_pr_data_ff\n\n# Add precision values for other models\nall_pr_data$gbm_precision_fc &lt;- gbm_pr_data_fc$precision\nall_pr_data$gbm_precision_cc &lt;- gbm_pr_data_cc$precision\n\n# Calculate precision differences relative to boosted trees\nall_pr_data$fc_precision_difference &lt;- gbm_pr_data_fc$precision - all_pr_data$precision\nall_pr_data$cc_precision_difference &lt;- gbm_pr_data_cc$precision - all_pr_data$precision\n\n# ==============================================================================\n# PLOT 1: PRECISION-RECALL CURVES FOR ALL MODELS\n# ==============================================================================\n\n#pdf(\"undocumented_pr_curves.pdf\", family=\"Times\", width=9)\npng(\"fffccc_pr_curves.png\", width = 22*100, height = 16*100, res = 300, family = \"Times\")\n\nop &lt;- par(family = \"serif\")\n\nplot(all_pr_data$recall, all_pr_data$precision, type = \"l\", pch = 19,\n     col = \"yellow\", xlab = \"Recall\", ylab = \"Precision\",\n     lwd = 3, ylim=c(0, 1), xlim = c(0,1), lty = 1, cex.lab=1.5, cex.axis=1)\n\nlines(all_pr_data$recall, all_pr_data$gbm_precision_fc, type = \"l\", \n      col = \"purple\", lwd = 3, lty = 2)\nlines(all_pr_data$recall, all_pr_data$gbm_precision_cc, type = \"l\", \n      col = \"pink\", lwd = 3, lty = 3)\n\nlegend(x = 0.5, y = 1, \n       col = c(\"white\", \"yellow\", \"purple\", \"pink\"),\n       lwd = c(3, 3, 3, 3, 3, 3), \n       lty = c(0, 1, 2, 3, 4, 5),\n       legend = c(\"Model:\", \"Full to Full\", \"Full to College\", \"College to College\"),\n       bty = 'n', cex = 1, ncol = 1, seg.len = 6)\n\n# Add random baseline\ntotal_undocu &lt;- sum(test_gbm_num$undocu_likely == 1)\nrandom_baseline &lt;- total_undocu / nrow(test_gbm_num)\nabline(h = random_baseline, lty = 2, col = \"gray\")\n# Add text label for random baseline\ntext(0.5, random_baseline + 0.05, \n     paste(\"Random baseline:\", round(random_baseline, 3)), \n     cex = 0.9, col = \"gray30\")\n\ndev.off()\n\n\npng \n  2 \n\n\nCode\n### Relative Pr-Re curves ###\n\npng(\"undocumented_sample_differences.png\", width = 22*100, height = 16*100, res = 300, family = \"Times\")\n\nop &lt;- par(family = \"serif\")\n\nplot(all_pr_data$recall, all_pr_data$fc_precision_difference, type = \"l\", pch = 19, \n     col = \"purple\", xlab = \"Recall Relative to Full-to-Full mapping\", ylab = \"Precision\", \n     lwd = 3, ylim = c(-0.3, 0.1), xlim = c(0,1), lty = 2, cex.lab=1.5, cex.axis=1)\n\nlines(all_pr_data$recall, all_pr_data$cc_precision_difference, type = \"l\", \n      col = \"pink\", lwd = 3, lty = 3)\n\nlegend(x = 0.5, y = -0.1, \n       col = c(\"white\", \"purple\", \"pink\"),\n       lwd = c(3, 3, 3), \n       lty = c(0, 2, 3),\n       legend = c(\"Model:\", \"Full-to-College\", \"College-to-College\"),\n       bty = 'n', cex = 1, ncol = 1, seg.len = 5)\n\nabline(h = 0, lty = 2, col = \"gray\")\n# Add text label for random baseline\ntext(0.5, 0.05, \n     paste(\"Full-to-Full baseline:\", round(0, 3)), \n     cex = 0.9, col = \"gray30\")\ndev.off()\n\n\npng \n  2"
  },
  {
    "objectID": "leading_model.html#demographic-precision-recall-curve-gbm-library-figure-3",
    "href": "leading_model.html#demographic-precision-recall-curve-gbm-library-figure-3",
    "title": "Leading ML Model",
    "section": "4.3 Demographic Precision-Recall Curve (GBM Library; Figure 3)",
    "text": "4.3 Demographic Precision-Recall Curve (GBM Library; Figure 3)\n\n\nCode\n# Total undocumented cases in test set\ntotal_undocu_gbm &lt;- sum(test_gbm_num$undocu_likely == 1)\n\n# Group 1: Top 10 states\nrecall_top_ten_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$top_ten_states == 1) / total_undocu_gbm\nprecis_top_ten_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$top_ten_states == 1) / \n                sum(test_gbm_num$top_ten_states == 1)\n\n# Group 2: Hispanic/Latino\nrecall_hispanic_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$spanish_hispanic_latino == 1) / total_undocu_gbm\nprecis_hispanic_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$spanish_hispanic_latino == 1) / \n                   sum(test_gbm_num$spanish_hispanic_latino == 1)\n\n# Group 3: Central American Latino\nrecall_central_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & \n                           test_gbm_num$central_latino ==   1) / total_undocu_gbm\nprecis_central_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & \n                           test_gbm_num$central_latino == 1) / \n                        sum(test_gbm_num$central_latino == 1)\n\n# Group 4: Asian-born\nrecall_asia_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$bpl_asia == 1) / total_undocu_gbm\nprecis_asia_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$bpl_asia == 1) / \n               sum(test_gbm_num$bpl_asia == 1)\n\n# Group 5: Non-fluent English speakers\nrecall_nonfluent_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$nonfluent == 1) / total_undocu_gbm\nprecis_nonfluent_gbm &lt;- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$nonfluent == 1) / \n                    sum(test_gbm_num$nonfluent == 1)\n\n\n# Set output file\nsetwd(figures_path)\npng(\"undocumented_demographic__gbm_pr_curve.png\", width = 2700, height = 2100, res = 300, family = \"Times\")\n# Or use: pdf(\"undocumented_demographic_pr_curve.pdf\", family=\"Times\", width=9)\n\n# Set up plot parameters\nop &lt;- par(family = \"serif\")\n\n# Main precision-recall curve\nplot(gbm_pr_data_ff$recall, gbm_pr_data_ff$precision, type = \"l\", pch = 19,\n     col = \"black\", xlab = \"Recall\", ylab = \"Precision\",\n     lwd = 3,\n     ylim = c(0, 1), xlim = c(0, 1), lty = 1, \n     cex.lab = 1.5, cex.axis = 1,\n     main = \"\")  # Add title if desired\n\n# Add demographic group points\npoints(recall_top_ten_gbm, precis_top_ten_gbm, pch = 17, col = 1, cex = 1.5)\npoints(recall_hispanic_gbm, precis_hispanic_gbm, pch = 19, col = 1, cex = 1.5)\npoints(recall_central_gbm, precis_central_gbm, pch = 19, col = \"gray\", cex = 1.5)\npoints(recall_asia_gbm, precis_asia_gbm, pch = 17, col = \"gray\", cex = 1.5)\npoints(recall_nonfluent_gbm, precis_nonfluent_gbm, pch = 15, col = \"darkgray\", cex = 1.5)\n\n\n# Demographic groups legend\nlegend(0.25, 1, \n       col = c(\"white\", 1, 1, \"gray\", \"gray\", \"darkgray\"), \n       pch = c(19, 17, 19, 19, 17, 15),\n       pt.cex = 1.5,\n       legend = c(\"Demographic Groups:\", \n                 \"Top 10 states\", \n                 \"Hispanic/Latino\",\n                 \"Central Latino\", \n                 \"Asian Born\",\n                 \"Non-fluent English\"),\n       bty = 'n', \n       cex = 1.25, \n       ncol = 1, \n       bg = \"white\")\n\n# Add random baseline\nrandom_baseline_gbm &lt;- sum(test_gbm_num$undocu_likely == 1) / nrow(test_gbm_num)\nabline(h = random_baseline_gbm, lty = 2, col = \"gray50\")\n\n# Add text label for random baseline\ntext(0.5, random_baseline_gbm + 0.05, \n     paste(\"Random Baseline:\", round(random_baseline_gbm, 3)), \n     cex = 0.9, col = \"gray30\")\n\ndev.off()\n\n\npng \n  2 \n\n\nCode\ncat(\"\\n=== DEMOGRAPHIC GROUP PERFORMANCE ===\\n\")\n\n\n\n=== DEMOGRAPHIC GROUP PERFORMANCE ===\n\n\nCode\ncat(\"Random Baseline Precision:\", round(random_baseline_gbm, 4), \"\\n\\n\")\n\n\nRandom Baseline Precision: 0.3037 \n\n\nCode\nsummary_df_gbm &lt;- data.frame(\n  Group = c(\"Top 10 states\", \"Hispanic/Latino\", \"Central Latino\", \n            \"Asian Born\", \"Non-fluent English\"),\n  Recall = c(recall_top_ten_gbm, recall_hispanic_gbm, recall_central_gbm, \n            recall_asia_gbm, recall_nonfluent_gbm),\n  Precision = c(precis_top_ten_gbm, precis_hispanic_gbm, precis_central_gbm, \n               precis_asia_gbm, precis_nonfluent_gbm)\n)\n\nprint(summary_df_gbm)\n\n\n               Group    Recall Precision\n1      Top 10 states 0.0000000       NaN\n2    Hispanic/Latino 0.6371882 0.3881215\n3     Central Latino 0.6167800 0.4047619\n4         Asian Born 0.1201814 0.1766667\n5 Non-fluent English 0.4693878 0.3670213\n\n\nCode\n# Save as both PDF and PNG\ncreate_demographic_plot_gbm &lt;- function(filename_base) {\n  # Function to create the plot (reusable)\n  make_plot &lt;- function() {\n    par(family = \"serif\")\n    plot(gbm_pr_data_ff$recall, gbm_pr_data_ff$precision, type = \"l\", pch = 19,\n         col = \"black\", xlab = \"Recall\", ylab = \"Precision\",\n         lwd = 3, ylim = c(0, 1), xlim = c(0, 1), lty = 1, \n         cex.lab = 1.5, cex.axis = 1)\n    \n    points(recall_top_ten, precis_top_ten, pch = 17, col = 1, cex = 1.5)\n    points(recall_hispanic, precis_hispanic, pch = 19, col = 1, cex = 1.5)\n    points(recall_central, precis_central, pch = 19, col = \"gray\", cex = 1.5)\n    points(recall_asia, precis_asia, pch = 17, col = \"gray\", cex = 1.5)\n    \n    legend(0.63, 1, col = c(\"white\", 1), lwd = 2, lty = c(0, 1),\n           legend = c(\"Learning Method:\", \"Boosted Trees\"),\n           bty = 'n', cex = 1.25, ncol = 1)\n    \n    legend(0.30, 1, col = c(\"white\", 1, 1, \"gray\", \"gray\"), \n           pch = c(19, 17, 19, 19, 17), pt.cex = 1.5,\n           legend = c(\"Groups:\", \"Top 10 states\", \"Hispanic/Latino\",\n                     \"Central Latino\", \"Asian Born\"),\n           bty = 'n', cex = 1.25, ncol = 1, bg = \"white\")\n    \n    abline(h = random_baseline_gbm, lty = 2, col = \"gray50\")\n  }\n  \n  # Save as PDF\n  pdf(paste0(filename_base, \".pdf\"), family = \"Times\", width = 9)\n  make_plot()\n  dev.off()\n  \n  # Save as PNG\n  png(paste0(filename_base, \".png\"), width = 2700, height = 2100, res = 300, family = \"Times\")\n  make_plot()\n  dev.off()\n}"
  },
  {
    "objectID": "freelance.html",
    "href": "freelance.html",
    "title": "Freelancing",
    "section": "",
    "text": "Some of the statistical analysis services I offer are:\n✅Linear/nonlinear regression, probability modeling (LPM, logistic)\n✅ A/B tests, T-tests, Chi-squared Tests, Hausman test, etc.\n✅Panel (longitudinal) data analysis (Random/fixed/mixed effects)\n✅Clustering, population weights, standardized coefficients\n✅Event study / Difference-in-Differences (DiD)\n✅Machine learning in R (logistic, KNN, Random Forests, etc.)\nI also offer the following services:\n✅ Tutoring (Statistics, Economics, and Mathematics)\n✅Consultations in data governance and dashboards\nBeyond college work and capstone projects (econometrics of Airbnb prices in Mexico using R; analysis of H-1B sponsor rates by industry type and location), I have also worked as a data analytics intern and as a research fellow (for which my research extended past the program). I have also taken courses in Coursera by IBM on data analysis (Python, Excel, AI), and have received certificates of completion.\nIn a year-long data analytics intern position, I learned how to create and implement a database into our center’s initiative on analyzing academic technology usage. I reached out to technology vendors of the university, requesting data extracts and communicating with them on the interpretation of data. I streamlined data collection, and organized all data into a database that is still used at the center. I cleaned and prepared several spreadsheets with hundreds of thousands of rows within Google Sheets, and provided key metrics through dashboards and reports in Google Looker Studio. I connected this data and made it accessible through Slack. I reported on my progress through Slack, Asana, and Zoom meetings (disseminating findings through bi-weekly meetings). I implemented data governance policies and facilitated meetings on data governance. I wrote a document on data governance. I wrote, recorded, and hosted trainings for my colleagues in Looker Studio. Overall, I became an expert at Google Sheets, Looker Studio, and communicating data analyses through visualizations. Providing data-driven insights through story-telling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mario Arce Acosta",
    "section": "",
    "text": "Hello, nice to meet you! My name is Mario, and my academic background consists of a B.S. in Applied Mathematics/Economics and a Minor in Business Analytics. I am currently studying Statistics for a Master’s in Science program at the University of California, Davis. I am proud to state that I presented my research at the Population Association of America 2025 Annual Conference, and that I will be presenting research at the 2025 Association for Public Policy Analysis & Management Conference!"
  },
  {
    "objectID": "msrip_symposium.html",
    "href": "msrip_symposium.html",
    "title": "MSRIP Symposium",
    "section": "",
    "text": "University of California, Riverside\n\n\n\n\n\n\nPoster: Summer research poster summarizing reseach initiative with Dr. Sovero\n\n\n\n\n\n\n\nImage of me in front of my poster\n\n\n\n\n\n\n\n\n\nImage with some peers I met through the program\n\n\n\n\n\n\n\nImage where I am presenting my research to a peer.\n\n\n\n\n\nWe used an extract from the American Community Survey (ACS) and Stata’s statistical software to examine educational mismatch rates of DACA eligible college graduates and the wage penalty they receive for their legal status."
  },
  {
    "objectID": "msrip_symposium.html#mind-the-gap-msrip-ucr-summer-research-programsymposium",
    "href": "msrip_symposium.html#mind-the-gap-msrip-ucr-summer-research-programsymposium",
    "title": "MSRIP Symposium",
    "section": "",
    "text": "University of California, Riverside\n\n\n\n\n\n\nPoster: Summer research poster summarizing reseach initiative with Dr. Sovero\n\n\n\n\n\n\n\nImage of me in front of my poster\n\n\n\n\n\n\n\n\n\nImage with some peers I met through the program\n\n\n\n\n\n\n\nImage where I am presenting my research to a peer.\n\n\n\n\n\nWe used an extract from the American Community Survey (ACS) and Stata’s statistical software to examine educational mismatch rates of DACA eligible college graduates and the wage penalty they receive for their legal status."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Portfolio",
    "section": "",
    "text": "Below are case studies demonstrating my work in Econometrics and Data Science."
  },
  {
    "objectID": "projects.html#selected-works",
    "href": "projects.html#selected-works",
    "title": "Portfolio",
    "section": "",
    "text": "Below are case studies demonstrating my work in Econometrics and Data Science."
  },
  {
    "objectID": "stata_analysis.html",
    "href": "stata_analysis.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "clear matrix\nclear mata\nclear\nset more off\nset scheme s1color\n\nset maxvar 30000\nset matsize 11000\nset segmentsize 2g\nset memory 8g\nmata: mata set matafavor speed\n\n\n// Set paths/directories here\n\nglobal drive \"/Users/verosovero/Library/CloudStorage/GoogleDrive-vsovero@ucr.edu\" //Sovero drive\n\n\nglobal main \"$drive/Shared drives/Undocu Research\"\ncd \"$main\"\n\nuse \"Data/EO_Final\", clear\n\n\nglobal covars hisp asian black male gov_worker bpl_foreign immig_by_ten nonfluent yrsed  metropolitan medicaid \nglobal undocu_vars undocu gbm_high_prob gbm_high_recall gbm_low_prob \n********************************************************************************\n***************** Undocumented Individual Mismatch regressions *****************\n********************************************************************************\n// Vertical Mismatch \n\neststo clear\nset more off\n\n***Logical edits V. mismatch model***\nreghdfe vmismatched hundermatched hovermatched undocu   $covars  [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo logical_vmismatch\n\nreghdfe vmismatched hundermatched hovermatched gbm_high_prob   $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmhigh_vmismatch\n\nreghdfe vmismatched hundermatched hovermatched gbm_high_recall  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmrecall_vmismatch\n\nreghdfe vmismatched hundermatched hovermatched gbm_low_prob  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmlow_vmismatch\n\nesttab logical_vmismatch gbmhigh_vmismatch gbmrecall_vmismatch gbmlow_vmismatch ///\n    using \"Output/Tables/vmismatch_regressions_ml.tex\", replace ///\n    label booktabs ///\n    drop($covars _cons) ///\n    rename(hundermatched \"Horizontal Undermatch\" ///\n           hovermatched \"Horizontal Overmatch\" ///\n           undocu \"Undocumented\" ///\n           gbm_high_prob \"Undocumented\" ///\n           gbm_high_recall \"Undocumented\" ///\n           gbm_low_prob \"Undocumented\") ///\n    stats(ymean r2 N, labels(\"Mean of Dep. Var.\" \"R-squared\" \"N\") fmt(%9.2f %9.2f %9.0fc)) ///\n    title(\"Regressions of Undocumented Status on Vertical Mismatch\") ///\n    mlabel(\"Logical Edits\" \"High Prob\" \"High Recall\" \"Low Prob\") ///\n    r2(4) b(4) se(4) brackets star(* .1 ** .05 *** .01) ///\n    note(\"Additional controls include:\") ///\n    addn(\"dummy age indicators, gender, Medicaid receipt, race/ethnicity, metropolitan residence,\" ///\n         \" government occupation, English-speaking fluency, foreign born, immigration by age 10,\" ///\n         \" broad degree category indicators, years of schooling, and state×year interaction fixed effects.\" ///\n         \" Robust standard errors are clustered by state.\")\n\n\n    \n\n// Horizontal Undermatch \nclear matrix\nset more off\neststo clear\n\nreghdfe hundermatched vmismatched  undocu   $covars  [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo logical_hunder\n\nreghdfe hundermatched vmismatched  gbm_high_prob   $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmhigh_hunder\n\nreghdfe hundermatched vmismatched  gbm_high_recall  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmrecall_hunder\n\nreghdfe hundermatched vmismatched  gbm_low_prob  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmlow_hunder\n\nesttab logical_hunder gbmhigh_hunder gbmrecall_hunder gbmlow_hunder ///\n    using \"Output/Tables/hundermatch_regressions_ml.tex\", replace ///\n    label booktabs ///\n     drop($covars _cons ) ///\n    rename(hundermatched \"Horizontal Undermatch\" ///\n           hovermatched \"Horizontal Overmatch\" ///\n           undocu \"Undocumented\" ///\n           gbm_high_prob \"Undocumented\" ///\n           gbm_high_recall \"Undocumented\" ///\n           gbm_low_prob \"Undocumented\") ///\n    stats(ymean r2 N, labels(\"Mean of Dep. Var.\" \"R-squared\" \"N\") fmt(%9.2f %9.2f %9.0fc)) ///\n    title(\"Regressions of Undocumented Status on Horizontal Undermatch\") ///\n    mlabel(\"Logical Edits\" \"High Prob\" \"High Recall\" \"Low Prob\") ///\n    r2(4) b(4) se(4) brackets star(* .1 ** .05 *** .01) ///\n    note(\"Additional controls include:\") ///\n    addn(\"dummy age indicators, gender, Medicaid receipt, race/ethnicity, metropolitan residence,\" ///\n         \" government occupation, English-speaking fluency, foreign born, immigration by age 10,\" ///\n         \" broad degree category indicators, years of schooling, and state×year interaction fixed effects.\" ///\n         \" Robust standard errors are clustered by state.\")\n\n    \n\n\n**********************************************************************************  \n****************Wage models with demographic columns/samples**************************\n**************************************************************************************\nclear matrix\nset more off\neststo clear\n\nreghdfe ln_adj vmismatched hundermatched hovermatched undocu   $covars  [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo logical_wage \n\nreghdfe ln_adj vmismatched hundermatched hovermatched gbm_high_prob   $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmhigh_wage \n\nreghdfe ln_adj vmismatched hundermatched hovermatched gbm_high_recall  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmrecall_wage\n\nreghdfe ln_adj vmismatched hundermatched hovermatched gbm_low_prob  $covars    [pweight=perwt] , absorb(statefip##year age  degfield_broader) vce(cluster statefip)\nestadd ysumm\neststo gbmlow_wage\n\nesttab logical_wage gbmhigh_wage gbmrecall_wage gbmlow_wage ///\n    using \"Output/Tables/wage_regressions_ml.tex\", replace ///\n    label booktabs ///\n      drop($covars _cons) ///\n    rename(hundermatched \"Horizontal Undermatch\" ///\n           hovermatched \"Horizontal Overmatch\" ///\n           undocu \"Undocumented\" ///\n           gbm_high_prob \"Undocumented\" ///\n           gbm_high_recall \"Undocumented\" ///\n           gbm_low_prob \"Undocumented\") ///\n stats(ymean r2 N, labels(\"Mean of Dep. Var.\" \"R-squared\" \"N\") fmt(%9.2f %9.2f %9.0fc)) ///\n    title(\"Regressions of Undocumented Status on Log Wages\") ///\n    mlabel(\"Logical Edits\" \"High Prob\" \"High Recall\" \"Low Prob\") ///\n    r2(4) b(4) se(4) brackets star(* .1 ** .05 *** .01) ///\n    note(\"Additional controls include:\") ///\n    addn(\"dummy age indicators, gender, Medicaid receipt, race/ethnicity, metropolitan residence,\" ///\n         \" government occupation, English-speaking fluency, foreign born, immigration by age 10,\" ///\n         \" broad degree category indicators, years of schooling, and state×year interaction fixed effects.\" ///\n         \" Robust standard errors are clustered by state.\")\n**Coefficient Plots****\n/* \n*vertical mismatch\ncoefplot (logical_vmismatch, label(Logical Edits) ) (knn_vmismatch, label(KNN) ) (rf_vmismatch, label(Random Forest) ) ///\n ||, drop($covars hundermatched hovermatched )  xline(0)   ///\nrename(elig_rf = \"DACA-eligible\" elig= \"DACA-eligible\" elig_knn= \"DACA-eligible\") ///\n xline(0) title(\"Vertical Mismatch\")\n  graph save elig_ipc_vmismatch, replace\n\n \n *horizontal undermatch\ncoefplot (logical_hunder, label(Logical Edits) ) (knn_hunder, label(KNN) ) (rf_hunder, label(Random Forest) ) ///\n ||, drop($covars vmismatched)  xline(0)   ///\nrename(elig_rf = \"DACA-eligible\" elig= \"DACA-eligible\" elig_knn= \"DACA-eligible\") ///\n xline(0) title(\"Horizontal Undermatch\")\n   graph save elig_ipc_hunder, replace\n\n *wages\ncoefplot (logical_wage, label(Logical Edits) ) (knn_wage, label(KNN) ) (rf_wage, label(Random Forest)) ///\n||, drop($covars vmismatched hundermatched hovermatched)  xline(0)   ///\nrename(elig_rf = \"DACA-eligible\" elig= \"DACA-eligible\" elig_knn= \"DACA-eligible\") ///\n xline(0) title(\"Log Wage\")\n   graph save elig_ipc_wage, replace\n\n*all together\ncoefplot (logical_vmismatch, label(Logical Edits) ) (knn_vmismatch, label(KNN) ) (rf_vmismatch, label(Random Forest) ), bylabel(Vertical Mismatch)  ///\n||  (logical_hunder, label(Logical Edits) ) (knn_hunder, label(KNN) ) (rf_hunder, label(Random Forest) ), bylabel(Horizontal Undermatch) ///\n||  (logical_wage, label(Logical Edits) ) (knn_wage, label(KNN) ) (rf_wage, label(Random Forest)), bylabel(Log Wage) ///\n||, drop($covars vmismatched hundermatched hovermatched)  xline(0)  ///\nrename(elig_rf = \"DACA-eligible\" elig= \"DACA-eligible\" elig_knn= \"DACA-eligible\") \ngraph export elig_ipc_coeff.png, replace            \n    \n*/"
  }
]