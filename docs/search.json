[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Using Machine Learning to Estimate the Effect of Undocumented Status on Education-Occupation Mismatch for College Graduates",
    "section": "",
    "text": "1 Libraries/Packages\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nCode\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(xtable)\nlibrary(ROCR)\nlibrary(class)\nlibrary(caTools)\nlibrary(rpart)  ## recursive partitioning\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nCode\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\nlibrary(ranger)\n#library(doParallel)    ## Optional: for faster RF model training\n\n\n\n\n2 Loading Data\n\n\nCode\ndata_path &lt;- \"G:/Shared drives/Undocu Research/Data\"\nfigures_path &lt;- \"G:/Shared drives/Undocu Research/Output/Figures\"\nsetwd(data_path)\n\nlibrary(readr)\nsipp08_2 &lt;- read_csv(\"(Step 1 output) Core_TM SIPP 2008 Wave 2.csv\")\n\n\nRows: 97929 Columns: 66\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (52): ssuid, tfipsst, efspouse, eentaid, epppnum, esex, erace, eorigin, ...\ndbl (14): rhcalyr, shhadid, ehrefper, ehhnumpp, thearn, rhpov, rfnkids, ebmn...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nView(sipp08_2)\n\n\n\n\n3 SIPP Data Cleaning and Recoding\n\n\nCode\nsipp08_2 &lt;- sipp08_2 %&gt;%\n  mutate(\n    undocu_entry = as.factor(ifelse(timstat==\"Other\", 1, 0)),\n    undocu_likely = as.factor(ifelse(timstat==\"Other\" & eadjust==\"No\", 1, 0)),\n    education = case_when(\n      eeducate == \"10th Grade\"  | eeducate == \"11th Grade\" | eeducate == \"12th grade, no diploma\" | eeducate == \"1st, 2nd, 3rd, or 4th grade\" | eeducate == \"5th Or 6th Grade\" | eeducate == \"7th Or 8th Grade\" | eeducate == \"9th Grade\" | eeducate == \"Less Than 1st Grade\"~ \"No HS diploma\",\n      eeducate == \"Diploma or certificate from a\" | eeducate == \"High School Graduate - (diploma\" ~ \"HS diploma\",\n      eeducate == \"Some college, but no degree\" ~ \"Some college\",\n      eeducate ==\"Associate (2-yr) college degree\" ~ \"Associate's\",\n      eeducate == \"Bachelor's degree (for example:\" ~ \"Bachelor's\",\n      eeducate == \"Master's degree (For example: MA,\" ~ \"Master's\",\n      eeducate == \"Doctorate degree (for example:\" ~ \"PhD\",\n      TRUE ~ \"Unknown\" # Default case\n    ),\n    yrsed = case_when(\n      eeducate == \"10th Grade\"~10,\n      eeducate == \"11th Grade\"~11,\n      eeducate == \"12th grade, no diploma\" | eeducate == \"Diploma or certificate from a\" | eeducate == \"High School Graduate - (diploma\" | eeducate == \"Some college, but no degree\" ~12,\n      eeducate == \"1st, 2nd, 3rd, or 4th grade\"~2.5,\n      eeducate == \"5th Or 6th Grade\"~5.5,\n      eeducate == \"7th Or 8th Grade\"~7.5,\n      eeducate == \"9th Grade\"~9,\n      eeducate == \"Less Than 1st Grade\"~0,\n      eeducate ==\"Associate (2-yr) college degree\"~14,\n      eeducate == \"Bachelor's degree (for example:\"~16,\n      eeducate == \"Master's degree (For example: MA,\"~17.5,\n      eeducate == \"Doctorate degree (for example:\"~22,\n      eeducate == \"Professional School degree (for\"~16,\n      TRUE ~ NA\n    ),\n    college = as.factor(ifelse(eeducate==\"Bachelor's degree (for example:\" | eeducate==\"Master's degree (For example: MA,\" | eeducate==\"Doctorate degree (for example:\", 1, 0)),\n    hs_only = as.factor(ifelse(eeducate==\"Some college, but no degree\" | eeducate== \"Associate (2-yr) college degree\" | eeducate==\"High School Graduate - (diploma\" | eeducate==\"Diploma or certificate from a\", 1, 0)),\n    immig_yr = case_when(\n      tmoveus == \"1961\"~1961,\n      tmoveus == \"1961-1968\"~1966,\n      tmoveus == \"1969-1973\"~1971,\n      tmoveus == \"1974-1978\"~1976,\n      tmoveus == \"1979-1980\"~1980,\n      tmoveus == \"1981-1983\"~1982,\n      tmoveus == \"1984-1985\"~1984,\n      tmoveus == \"1986-1988\"~1987,\n      tmoveus == \"1989-1990\"~1989,\n      tmoveus == \"1991-1992\"~1991,\n      tmoveus == \"1993-1994\"~1993,\n      tmoveus == \"1995-1996\"~1995,\n      tmoveus == \"1997-1998\"~1998,\n      tmoveus == \"1999\"~1999,\n      tmoveus == \"2000\"~2000,\n      tmoveus == \"2001\"~2001,\n      tmoveus == \"2002-2003\"~2002,\n      tmoveus == \"2004\"~2004,\n      tmoveus == \"2005\"~2005,\n      tmoveus == \"2006\"~2006,\n      tmoveus == \"2007\"~2007,\n      tmoveus == \"2008-2009\"~2009,\n      TRUE ~ 0 # Default case\n    ),\n    married = as.factor(ifelse(ems==\"Married, spouse absent\" | ems==\"Married, spouse present\", 1, 0)),\n    english_difficult = as.factor(ifelse(ehowwell==\"Not at all\" | ehowwell==\"Not well\", 1, 0)),\n    nonfluent = as.factor(ifelse(ehowwell==\"Not at all\" | ehowwell==\"Not well\", 1, 0)),\n    english_home = as.factor(ifelse(tlang1==\"Not in Universe\", 1, 0)),\n    spanish_hispanic_latino = as.factor(ifelse(eorigin==\"Yes\", 1, 0)),\n    medicaid = as.factor(ifelse(rcutyp57==\"Yes, covered\", 1, 0)),\n    household_size = ehhnumpp,\n    race = case_when(\n      erace==\"Asian alone\" ~ \"Asian\",\n      erace==\"Black alone\" ~ \"Black\",\n      erace==\"White alone\" ~ \"White\",\n      erace==\"Residual\" ~ \"Other\",\n      TRUE ~ \"Unknown\"\n    ),\n    fem = as.factor(ifelse(esex==\"Female\", 1, 0)),\n    asian = as.factor(ifelse(erace==\"Asian alone\", 1, 0)),\n    black = as.factor(ifelse(erace==\"Black alone\", 1, 0)),\n    white = as.factor(ifelse(erace==\"White alone\", 1, 0)),\n    other_race = as.factor(ifelse(erace==\"Residual\", 1, 0)),\n    employed = as.factor(ifelse(rmesr==\"With a job at least 1 but not all\" | rmesr==\"With a job entire month, absent\" | rmesr==\"With a job entire month, worked\", 1, 0)),\n    years_us = rhcalyr - immig_yr,\n    citizen = as.factor(ifelse(ecitizen==\"Yes\", 1, 0)),\n    cit_spouse = as.factor(cit_spouse),\n    poverty = as.factor(ifelse(thearn&lt;rhpov, 1, 0)),\n    armed_forces = as.factor(ifelse(eafnow==\"Yes\" | eafever==\"Yes\", 1, 0)),\n    health_ins= as.factor(ifelse(rcutyp57==\"Yes, covered\" | rcutyp58==\"Yes, covered\" , 1, 0)),\n    medicare = as.factor(ifelse(ecrmth==\"Yes, covered\", 1, 0)),\n    social_security = as.factor(ifelse(rcutyp01==\"Yes, covered\" | rcutyp03==\"Yes, covered\", 1, 0)),\n    central_latino = as.factor(ifelse(tbrstate==\"Central America\" & eorigin==\"Yes\", 1, 0)),\n    bpl_usa = as.factor(ifelse(ebornus==\"Yes\", 1, 0)),\n    bpl_asia = as.factor(ifelse(tbrstate == \"Eastern Asia\"| tbrstate == \"South Central Asia\"| tbrstate == \"South East Asia, West Asia,\", 1, 0)),\n    top_ten_states = as.factor(ifelse(tfipsst==\"California\" | tfipsst==\"Texas\" | tfipsst==\"Florida\" | tfipsst==\"New Jersey\" | tfipsst==\"Illinois\" | tfipsst==\"New York\" | tfipsst==\"North Carolina\" | tfipsst==\"Georgia\" | tfipsst==\"Washington\" | tfipsst==\"Arizona\", 1, 0))\n  )\n\nsipp08_2$bpl_foreign &lt;- as.factor(ifelse(sipp08_2$bpl_usa==1, 0, 1))\nsipp08_2$undocu_likely &lt;- replace(sipp08_2$undocu_likely, sipp08_2$immig_yr &lt;= 1961, 0)\nsipp08_2$years_us &lt;- ifelse(sipp08_2$years_us == 2008 | sipp08_2$years_us == 2009 | sipp08_2$years_us == -1 , NA, sipp08_2$years_us)\nsipp08_2$tage &lt;- replace(sipp08_2$tage, sipp08_2$tage == \"Less than 1 full year old\", 0)\nsipp08_2$age &lt;- as.numeric(sipp08_2$tage)\nsipp08_2$undocu_likely &lt;- replace(sipp08_2$undocu_likely, sipp08_2$armed_forces==1 | sipp08_2$social_security==1, 0 )\nsipp08_2$undocu_logical &lt;- as.factor(ifelse(sipp08_2$citizen==0 & (sipp08_2$armed_forces==0 | sipp08_2$medicare==0 | sipp08_2$social_security==0), 1, 0))\nsipp08_2$id &lt;- seq_len(nrow(sipp08_2))\n\n\n# Define column sets for different analyses\nvariable_lists &lt;- list(\n  base = c(\"undocu_likely\", \"central_latino\", \"bpl_asia\", \"medicaid\", \"age\", \"fem\", \n           \"married\", \"cit_spouse\", \"nonfluent\", \"spanish_hispanic_latino\", \n           \"household_size\", \"poverty\", \"asian\", \"black\", \"white\", \"other_race\", \n           \"employed\", \"years_us\", \"yrsed\"),\n  \n  descriptive = c(\"undocu_likely\", \"undocu_logical\", \"bpl_foreign\", \"medicaid\", \n                  \"central_latino\", \"bpl_asia\", \"age\", \"fem\", \"married\", \"cit_spouse\", \n                  \"nonfluent\", \"spanish_hispanic_latino\", \"household_size\", \"poverty\", \n                  \"asian\", \"black\", \"white\", \"other_race\", \"employed\", \"years_us\", \"yrsed\")\n\n)\n\n\n\n\n4 Subsamples\n\n\nCode\n# Create analysis datasets\ncreate_datasets &lt;- function(data, variables) {\n  datasets &lt;- list()\n  \n  # Filter for undocu_logical == 1\n  undocu_filter &lt;- data[data$undocu_logical == 1, ]\n  \n  datasets$dTable &lt;- as.data.frame(undocu_filter[, variables$descriptive]) %&gt;%\n    mutate_at(vars(-undocu_likely), as.numeric) %&gt;%\n    na.omit()\n  \n  datasets$sample &lt;- as.data.frame(undocu_filter[, variables$base]) %&gt;%\n    na.omit()\n  \n  datasets$knn &lt;- as.data.frame(undocu_filter[, variables$base]) %&gt;%\n    mutate_at(vars(-undocu_likely), as.numeric) %&gt;%\n    na.omit()\n  \n  \n  # Demographic subsets (college graduates only)\n  datasets$noncit &lt;- data[data$citizen == 0 & data$college == 1, ]\n  datasets$central_latino &lt;- data[data$central_latino == 1 & data$college == 1, ]\n  datasets$spanish_hispanic_latino &lt;- data[data$spanish_hispanic_latino == 1 & data$college == 1, ]\n  datasets$top_states &lt;- data[data$top_ten_states == 1 & data$college == 1, ]\n  \n  return(datasets)\n}\n\n# Create all analysis datasets\nall_data &lt;- create_datasets(sipp08_2, variable_lists)\n\nsetwd(\"G:/Shared drives/Undocu Research/Data\")\nwrite.csv(sipp08_2, \"SIPP08_2.csv\", row.names = FALSE)\n\n\n\n\n5 Logistic\n\n\nCode\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\n\nset.seed(1)\ntrain_index_logistic &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_logistic &lt;- all_data$sample[train_index_logistic, ]\ntest_logistic &lt;- all_data$sample[-train_index_logistic, ]\n\n## Create trainControl object\ncontrol &lt;- trainControl(\n    method = \"cv\",\n    number = 10,  \n    summaryFunction = twoClassSummary,\n    classProbs = TRUE,\n    sampling = \"up\"\n)\n\n## Train glm with custom trainControl\nlogistic_model &lt;- train(undocu_likely ~ age + fem + married + cit_spouse + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + medicaid + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, train_logistic,\n               method = \"glm\",\n               trControl = control,\n               metric = 'ROC')\n\np_logistic &lt;- predict(logistic_model, test_logistic)\n\n\n# Generate confusion matrix\nlogistic_matrix &lt;- confusionMatrix(p_logistic, test_logistic$undocu_likely, positive = \"X1\")\nprint(logistic_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 676 141\n        X1 335 300\n                                          \n               Accuracy : 0.6722          \n                 95% CI : (0.6474, 0.6963)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 0.978           \n                                          \n                  Kappa : 0.3104          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6803          \n            Specificity : 0.6686          \n         Pos Pred Value : 0.4724          \n         Neg Pred Value : 0.8274          \n             Prevalence : 0.3037          \n         Detection Rate : 0.2066          \n   Detection Prevalence : 0.4373          \n      Balanced Accuracy : 0.6745          \n                                          \n       'Positive' Class : X1              \n                                          \n\n\nCode\nsummary_logistic&lt;-summary(logistic_model)$coefficients[,c(1,4)]\nsummary_logistic\n\n\n                             Estimate     Pr(&gt;|z|)\n(Intercept)               1.086274760 2.403930e-05\nage                      -0.028615899 2.805518e-22\nfem1                     -0.107628855 1.062181e-01\nmarried1                 -0.059375631 3.964547e-01\ncit_spouse1              -0.220240756 4.178604e-03\nnonfluent1                0.292738271 1.213362e-04\nspanish_hispanic_latino1  0.024379055 8.948373e-01\ncentral_latino1           0.935938662 1.932298e-07\nbpl_asia1                -0.212503302 1.758857e-01\nmedicaid1                -0.394646972 1.362742e-04\nhousehold_size            0.006487852 7.258096e-01\npoverty1                  0.335671026 9.558128e-06\nasian1                   -0.014682482 9.441694e-01\nblack1                   -0.217738645 2.824772e-01\nwhite1                   -0.206672671 2.291539e-01\nemployed1                 0.104813209 1.496649e-01\nyears_us                 -0.041445286 3.879564e-20\nyrsed                     0.003842108 7.005681e-01\n\n\nCode\nxtable(summary_logistic, digits=4)\n\n\n\n  \n\n\n\nCode\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nlogistic.preds = predict(logistic_model, newdata = test_logistic, type = \"prob\")[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nlogistic.prediction = prediction(logistic.preds, test_logistic$undocu_likely)\n\nlogistic.pr = performance(logistic.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n\n\n\n6 KNN\n\n\nCode\nset.seed(1)\nlevels(all_data$knn$undocu_likely) &lt;- make.names(levels(all_data$knn$undocu_likely))\ntrain_index_knn &lt;- createDataPartition(all_data$knn$undocu_likely, p = 0.7, list = FALSE)\ntrain_knn &lt;- all_data$knn[train_index_knn, ]\ntest_knn &lt;- all_data$knn[-train_index_knn, ]\n\n\n\nknn_model &lt;- train(undocu_likely ~., data = train_knn, method = \"knn\", \n                       trControl = control, \n                       tuneLength = 10,\n                       metric = 'ROC',\n                       preProcess = c('center', 'scale'))\n\nknn_model\n\n\nk-Nearest Neighbors \n\n3391 samples\n  18 predictor\n   2 classes: 'X0', 'X1' \n\nPre-processing: centered (18), scaled (18) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 3051, 3052, 3052, 3052, 3052, 3052, ... \nAddtional sampling using up-sampling prior to pre-processing\n\nResampling results across tuning parameters:\n\n  k   ROC        Sens       Spec     \n   5  0.6541396  0.5872881  0.6207058\n   7  0.6635782  0.5974576  0.6430825\n   9  0.6688536  0.6000000  0.6488331\n  11  0.6799451  0.6118644  0.6644137\n  13  0.6834141  0.6067797  0.6625000\n  15  0.6891838  0.6038136  0.6585512\n  17  0.6862166  0.6110169  0.6566374\n  19  0.6831830  0.6008475  0.6498786\n  21  0.6892569  0.6088983  0.6606329\n  23  0.6887947  0.6059322  0.6683346\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 21.\n\n\nCode\npredict_knn &lt;- predict(knn_model, test_knn)\n\nknn_matrix &lt;- confusionMatrix(predict_knn, test_knn$undocu_likely, positive = \"X1\")\nprint(knn_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 637 137\n        X1 374 304\n                                          \n               Accuracy : 0.6481          \n                 95% CI : (0.6229, 0.6727)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2774          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6893          \n            Specificity : 0.6301          \n         Pos Pred Value : 0.4484          \n         Neg Pred Value : 0.8230          \n             Prevalence : 0.3037          \n         Detection Rate : 0.2094          \n   Detection Prevalence : 0.4669          \n      Balanced Accuracy : 0.6597          \n                                          \n       'Positive' Class : X1              \n                                          \n\n\nCode\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nknn.preds = predict(knn_model, newdata = test_knn, type = \"prob\")[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nknn.prediction = prediction(knn.preds, test_knn$undocu_likely)\n\nknn.pr = performance(knn.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n\n\nThe number of trees in the forest\nThe number of features to consider at any given split: \\(m_{try}\\)\nThe complexity of each tree\nThe sampling scheme\nThe splitting rule to use during tree construction\nand (2) typically have the largest impact on predictive accuracy and should always be tuned. (3) and (4) tend to have marginal impact on predictive accuracy but are still worth exploring. They also have the ability to influence computational efficiency. (5) tends to have the smallest impact on predictive accuracy and is used primarily to increase computational efficiency.\n\n\n\n7 RF\n\n\nCode\n### You may use the following code to speed up code being ran (Use all cores except one)\n# cl &lt;- makePSOCKcluster(detectCores() - 1) \n# registerDoParallel(cl)\n\nlevels(all_data$sample$undocu_likely) &lt;- make.names(levels(all_data$sample$undocu_likely))\n\n\ntrain_index_rf &lt;- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)\ntrain_rf &lt;- all_data$sample[train_index_rf, ]\ntest_rf &lt;- all_data$sample[-train_index_rf, ]\n\n\ntrain_rf &lt;- train_rf %&gt;%\n  select(-undocu_likely, undocu_likely)\n\n#Manual search by create 10 folds and repeat 3 times\n#control_rf &lt;- trainControl(method = 'repeatedcv',\n                        #number = 10,\n                       # repeats = 3,\n                        #search = 'grid')\n\ntunegrid &lt;- expand.grid(mtry = c(2,4,8,12),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = 1)\n\ncontrol_up &lt;- trainControl(\n    method = \"cv\",\n    number = 10,  \n    summaryFunction = twoClassSummary,\n    classProbs = TRUE,\n    sampling = \"up\",\n)\n\n\nset.seed(1)\nrf_model &lt;- train(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed,\n               data = train_rf,\n               method = \"ranger\",\n               trControl = control_up,\n               tuneLength = 5,\n               importance = \"impurity\",\n               metric = 'ROC')\nprint(rf_model)\n\n\nRandom Forest \n\n3391 samples\n  18 predictor\n   2 classes: 'X0', 'X1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 3052, 3052, 3051, 3052, 3052, 3052, ... \nAddtional sampling using up-sampling\n\nResampling results across tuning parameters:\n\n  mtry  splitrule   ROC        Sens       Spec     \n   2    gini        0.7531242  0.7135593  0.6586165\n   2    extratrees  0.7409146  0.6809322  0.6896098\n   6    gini        0.7496400  0.8088983  0.5102128\n   6    extratrees  0.7483606  0.7940678  0.5286034\n  10    gini        0.7421022  0.8233051  0.4947069\n  10    extratrees  0.7421380  0.8110169  0.4966019\n  14    gini        0.7317369  0.8063559  0.4888910\n  14    extratrees  0.7391064  0.8088983  0.4898058\n  18    gini        0.7297986  0.8101695  0.4889190\n  18    extratrees  0.7369086  0.8105932  0.4908140\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1.\n\n\nCode\nplot(rf_model)\n\n\n\n\n\n\n\n\n\nCode\np_rf &lt;- predict(rf_model, test_rf)\n\nrf_matrix &lt;- confusionMatrix(p_rf, test_rf$undocu_likely, positive=\"X1\")\nrf_matrix\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  X0  X1\n        X0 715 166\n        X1 296 275\n                                          \n               Accuracy : 0.6818          \n                 95% CI : (0.6572, 0.7057)\n    No Information Rate : 0.6963          \n    P-Value [Acc &gt; NIR] : 0.8897          \n                                          \n                  Kappa : 0.3054          \n                                          \n Mcnemar's Test P-Value : 1.954e-09       \n                                          \n            Sensitivity : 0.6236          \n            Specificity : 0.7072          \n         Pos Pred Value : 0.4816          \n         Neg Pred Value : 0.8116          \n             Prevalence : 0.3037          \n         Detection Rate : 0.1894          \n   Detection Prevalence : 0.3933          \n      Balanced Accuracy : 0.6654          \n                                          \n       'Positive' Class : X1              \n                                          \n\n\nCode\nfeature_importance &lt;- vip(rf_model, num_features = 19, bar = FALSE)\ngrid.arrange(feature_importance, nrow = 1)\n\n\n\n\n\n\n\n\n\nCode\nall_data$dTable$undocu_logit &lt;- predict(logistic_model, all_data$sample)\nall_data$dTable$undocu_knn &lt;- predict(knn_model, all_data$knn)\nall_data$dTable$undocu_rf &lt;- predict(rf_model, all_data$sample)\nsetwd(\"G:/Shared drives/Undocu Research/Data\")\nwrite.csv(all_data$dTable, \"SIPP_dTable.csv\", row.names = FALSE)\n\n\nrf_model$finalModel$num.trees\n\n\n[1] 500\n\n\nCode\n# Access specific parameters\nrf_optimal_trees &lt;- rf_model$bestTune$mtry\ncat(\"Optimal mtry:\", rf_optimal_trees, \"\\n\")\n\n\nOptimal mtry: 2 \n\n\nCode\n## Metrics for Figure 2 comparison\n# Make predictions on test set\nrf.preds = predict(rf_model, newdata = test_rf, type = \"prob\", n.trees = rf_optimal_trees)[, 2]  # Get probabilities for class \"1\"\n# Create prediction object for ROCR\nrf.prediction = prediction(rf.preds, test_rf$undocu_likely)\n\nrf.pr = performance(rf.prediction,\"prec\",\"rec\") # Precision-Recall curve\n\n# stopCluster(cl) # Optional code part of parallel processing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mario Arce Acosta",
    "section": "",
    "text": "Hello, nice to meet you! My name is Mario, and my academic background consists of a B.S. in Applied Mathematics/Economics and a Minor in Business Analytics. I am currently studying Statistics for a Master’s in Science program at the University of California, Davis. I am proud to state that I presented my research at the Population Association of America 2025 Annual Conference, and that I will be presenting research at the 2025 Association for Public Policy Analysis & Management Conference!\nSome of the statistical analysis services I offer are:\n✅Linear/nonlinear regression, probability modeling (LPM, logistic)\n✅ A/B tests, T-tests, Chi-squared Tests, Hausman test, etc.\n✅Panel (longitudinal) data analysis (Random/fixed/mixed effects)\n✅Clustering, population weights, standardized coefficients\n✅Event study / Difference-in-Differences (DiD)\n✅Machine learning in R (logistic, KNN, Random Forests, etc.)\nI also offer the following services:\n✅ Tutoring (Statistics, Economics, and Mathematics)\n✅Consultations in data governance and dashboards\nBeyond college work and capstone projects (econometrics of Airbnb prices in Mexico using R; analysis of H-1B sponsor rates by industry type and location), I have also worked as a data analytics intern and as a research fellow (for which my research extended past the program). I have also taken courses in Coursera by IBM on data analysis (Python, Excel, AI), and have received certificates of completion.\nIn a year-long data analytics intern position, I learned how to create and implement a database into our center’s initiative on analyzing academic technology usage. I reached out to technology vendors of the university, requesting data extracts and communicating with them on the interpretation of data. I streamlined data collection, and organized all data into a database that is still used at the center. I cleaned and prepared several spreadsheets with hundreds of thousands of rows within Google Sheets, and provided key metrics through dashboards and reports in Google Looker Studio. I connected this data and made it accessible through Slack. I reported on my progress through Slack, Asana, and Zoom meetings (disseminating findings through bi-weekly meetings). I implemented data governance policies and facilitated meetings on data governance. I wrote a document on data governance. I wrote, recorded, and hosted trainings for my colleagues in Looker Studio. Overall, I became an expert at Google Sheets, Looker Studio, and communicating data analyses through visualizations. Providing data-driven insights through story-telling."
  }
]